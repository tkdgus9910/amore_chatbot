# -*- coding: utf-8 -*-
"""
Created on Wed Sep 24 16:25:02 2025

@author: tmlab
"""

#%% 01. ì„ë² ë”© ë° DB ë¡œë“œ
# -*- coding: utf-8 -*-
import os
import requests
from functools import partial

# LangChain ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableLambda, RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# ================================================================
# 1. OpenRouter API ì„¤ì • ë° í˜¸ì¶œ í•¨ìˆ˜
# ================================================================

# â— ì¤‘ìš”: OpenRouter API í‚¤ë¥¼ ì„¤ì •í•˜ì„¸ìš”.
# os.environ["OPENROUTER_API_KEY"] = "sk-or-v1-..."
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")

OPENROUTER_BASE_URL = "https://openrouter.ai/api/v1"
HEADERS = {
    "Authorization": f"Bearer {OPENROUTER_API_KEY}",
    "Content-Type": "application/json"
}

def ask_openrouter(question: str, model: str, temperature: float = 0.1) -> str:
    """
    OpenAI í˜¸í™˜ chat/completions í˜•ì‹ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì–¸ì–´ ëª¨ë¸ì— ì§ˆì˜í•©ë‹ˆë‹¤.
    (RAG íŒŒì´í”„ë¼ì¸ì— í†µí•©í•˜ê¸° ìœ„í•´ íŒŒë¼ë¯¸í„° ìˆœì„œë¥¼ ì¡°ì •í–ˆìŠµë‹ˆë‹¤.)
    """
    url = f"{OPENROUTER_BASE_URL}/chat/completions"
    payload = {
        "model": model,
        "temperature": temperature,
        "messages": [
            {
                "role": "system",
                "content": "You are a helpful assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise. Answer in Korean."
            },
            {
                "role": "user",
                "content": question # RAG í”„ë¡¬í”„íŠ¸ê°€ í¬í•¨ëœ ì „ì²´ ì§ˆë¬¸ì´ ì´ê³³ìœ¼ë¡œ ì „ë‹¬ë©ë‹ˆë‹¤.
            }
        ]
    }
    
    if not OPENROUTER_API_KEY:
        return "ì˜¤ë¥˜: OpenRouter API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

    try:
        resp = requests.post(url, headers=HEADERS, json=payload, timeout=120)
        resp.raise_for_status()
        data = resp.json()
        return data["choices"][0]["message"]["content"].strip()
    except requests.exceptions.RequestException as e:
        return f"API ìš”ì²­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
    except (KeyError, IndexError):
        return f"API ì‘ë‹µ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {resp.text}"

#%% 02. retriever ì¤€ë¹„ 

# ================================================================
# 2. Retriever ì¤€ë¹„ (ê¸°ì¡´ ì½”ë“œ)
# ================================================================

# ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
print("ì„ë² ë”© ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
model_name = "nlpai-lab/KURE-v1"
model_kwargs = {'device': 'cpu'} # GPUê°€ ì—†ë‹¤ë©´ 'cpu'ë¡œ ë³€ê²½
encode_kwargs = {'normalize_embeddings': True}
embeddings = HuggingFaceEmbeddings(
    model_name=model_name,
    model_kwargs=model_kwargs,
    encode_kwargs=encode_kwargs
)

# ChromaDB ë¡œë“œ ë° Retriever ì„¤ì •

chroma_persist_dir = r"C:\Users\PC1\OneDrive\í”„ë¡œì íŠ¸\250801_ì•„ëª¨ë ˆ\chroma_db"
db = Chroma(persist_directory=chroma_persist_dir, embedding_function=embeddings)
retriever = db.as_retriever(search_kwargs={"k": 3})
print("DBë¥¼ Retrieverë¡œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.\n")

#%% 03. íŒŒì´í”„ë¼ì¸ êµ¬ì„±

# ================================================================
# 3. RAG íŒŒì´í”„ë¼ì¸ êµ¬ì„± (Context í¬í•¨í•˜ë„ë¡ ìˆ˜ì •)
# ================================================================

# 3-1. Prompt Template ë° LLM ì¤€ë¹„ (ì´ì „ê³¼ ë™ì¼)
template = """
ì£¼ì–´ì§„ ë§¥ë½(Context) ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì„¸ìš”.
ë§¥ë½ì—ì„œ ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´, "ì œê³µëœ ì •ë³´ë§Œìœ¼ë¡œëŠ” ë‹µë³€í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤."ë¼ê³  ë‹µí•˜ì„¸ìš”. ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”.

[ë§¥ë½]
{context}

[ì§ˆë¬¸]
{question}
"""
prompt = ChatPromptTemplate.from_template(template)
selected_model = "google/gemma-2-9b-it"
llm = RunnableLambda(lambda p: ask_openrouter(question=p.to_string(), model=selected_model))

# 3-2. â­ï¸â­ï¸â­ï¸ ìµœì¢… ì²´ì¸ êµ¬ì„± (ê°€ì¥ í° ë³€ê²½ì ) â­ï¸â­ï¸â­ï¸

# ê²€ìƒ‰ëœ ë¬¸ì„œ(context)ë¥¼ í›„ì† ì²´ì¸ì— ì „ë‹¬í•˜ëŠ” í•¨ìˆ˜
def format_docs(docs):
    return "\n\n".join(f"--- ë¬¸ì„œ {i+1} ---\n{doc.page_content}" for i, doc in enumerate(docs))

# 1. ì§ˆë¬¸ì„ ë°›ì•„ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ê³ , contextì™€ questionì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë§Œë“¦
setup_and_retrieval = RunnablePassthrough.assign(
    context=lambda x: format_docs(retriever.invoke(x["question"]))
)

# 2. contextì™€ questionì„ ë°›ì•„ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì²´ì¸
rag_chain_from_docs = (
    prompt
    | llm
    | StrOutputParser()
)

# 3. ìµœì¢…ì ìœ¼ë¡œ contextì™€ answerë¥¼ í•¨ê»˜ ë°˜í™˜í•˜ëŠ” ì²´ì¸
final_chain = setup_and_retrieval | RunnablePassthrough.assign(
    answer=rag_chain_from_docs
)

#%% 04.RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ì¶œë ¥ ë°©ì‹ ë³€ê²½)

# ================================================================
# 4. RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ì¶œë ¥ ë°©ì‹ ë³€ê²½)
# ================================================================
if __name__ == "__main__":
    query = 'ì½”ë¡œë‚˜19 ì´í›„ ì†Œë¹„ì íŠ¸ë Œë“œì— ëŒ€í•´ ì•Œë ¤ì¤˜'
    print(f"ì§ˆë¬¸: {query}\n")
    print("--- RAG íŒŒì´í”„ë¼ì¸ ë‹µë³€ ìƒì„± ì¤‘ ---")

    # ì²´ì¸ì„ ì‹¤í–‰í•˜ë©´ 'context'ì™€ 'answer'ê°€ í¬í•¨ëœ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜
    result = final_chain.invoke({"question": query})

    # ìµœì¢… ë‹µë³€ ì¶œë ¥
    print("\nâœ… [ìµœì¢… ë‹µë³€]")
    print(result["answer"])

    # ì°¸ê³ í•œ ì›ë¬¸(Context) ì¶œë ¥
    print("\n\nğŸ“š [ì°¸ê³  ì›ë¬¸]")
    print(result["context"])