# streamlit_rag_app.py

import os
import re
import sys
import shutil
import subprocess
import textwrap
import html
from time import sleep
from urllib.parse import unquote
import streamlit as st
from pathlib import Path
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
import tempfile
import json
import chromadb  # Chroma 1.x client

# -----------------------------
# ğŸ”§ ê¸°ë³¸ ì„¤ì •
# -----------------------------
try:
    BASE_DIR = Path(__file__).resolve().parent
except NameError:
    BASE_DIR = Path.cwd()

# temp í´ë”(ë‹¨ì¼ ì‚¬ìš©ì ì „ì œ) + í¬ì¸í„° ë°©ì‹
DEFAULT_CHROMA_DIR = str(Path(tempfile.gettempdir()) / "chroma_store")  # /tmp/chroma_store
DEFAULT_MODEL       = "google/gemma-2-9b-it"
EMBED_MODEL_NAME    = "nlpai-lab/KURE-v1"
OPENROUTER_BASE_URL = "https://openrouter.ai/api/v1"

SYSTEM_PROMPT = """You are a precise RAG assistant. Use ONLY the provided context.
If the answer is not in the context, say you don't know briefly.
Answer in Korean.
"""

USER_PROMPT_TEMPLATE = """[Context]
{context}

[Question]
{question}

[Instructions]
- Keep the answer concise but complete (â‰¤ 5 sentences).
"""

# -----------------------------
# ğŸ”§ í¬ì¸í„° ìœ í‹¸
# -----------------------------
def _pointer_file(root: str) -> Path:
    return Path(root) / "CURRENT.txt"

def _resolve_store_path(root: str) -> Path | None:
    p = _pointer_file(root)
    if not p.exists():
        return None
    try:
        sp = p.read_text(encoding="utf-8").strip()
    except Exception:
        return None
    return Path(sp) if sp and Path(sp).exists() else None

# -----------------------------
# ğŸ”§ ìœ í‹¸: ë Œë” í•¨ìˆ˜/íŒŒì„œ
# -----------------------------
def _clean_filename(name: str) -> str:
    if not name:
        return "unknown"
    return unquote(name).replace("+", " ")

def parse_summary_and_figures(text: str):
    if not text:
        return [], []
    lines = [l.strip() for l in text.splitlines() if l.strip()]
    summary_items, figure_rows = [], []
    in_summary = False

    for ln in lines:
        if ln.startswith("í•µì‹¬ìš”ì•½"):
            in_summary = True
            after = ln.split(":", 1)[1].strip() if ":" in ln else ""
            if after:
                for piece in after.split(" - "):
                    p = piece.strip(" -")
                    if p:
                        summary_items.append(p)
            continue
        if in_summary:
            if ln.startswith("í•µì‹¬ìˆ˜ì¹˜") or ln.startswith("ë©”ëª¨"):
                in_summary = False
                continue
            if ln.startswith("-"):
                summary_items.append(ln.lstrip("- ").strip())

    for ln in lines:
        if "ê°’:" in ln and "|" in ln:
            ln2 = re.sub(r'^\s*\d+\)\s*', '', ln)
            parts = [p.strip() for p in ln2.split("|")]
            row = {"ë¼ë²¨": "", "ê°’": "", "ë‹¨ìœ„": "", "ê¸°ê°„/ê¸°ì¤€": "", "ê·¼ê±°ì˜ì—­": ""}
            for p in parts:
                if ":" in p:
                    k, v = p.split(":", 1)
                    k = k.strip(); v = v.strip()
                    if k in row:
                        row[k] = v
            if row.get("ë¼ë²¨") or row.get("ê°’"):
                figure_rows.append(row)
    return summary_items[:5], figure_rows[:5]

def format_doc(doc, idx=None):
    meta = getattr(doc, "metadata", {}) or {}
    src  = meta.get("source") or meta.get("file") or meta.get("path") or "unknown"
    page = meta.get("page_number") or meta.get("page") or "N/A"
    txt  = (getattr(doc, "page_content", "") or "").strip()
    head = f"[{idx}] {os.path.basename(src)} (p.{page})" if idx is not None else f"{os.path.basename(src)} (p.{page})"
    body = textwrap.shorten(txt, width=500, placeholder=" â€¦")
    return head, body, meta

def render_doc_card(idx, doc):
    _, _, meta = format_doc(doc, idx)
    fname = _clean_filename(meta.get("source") or meta.get("file") or meta.get("path") or "unknown")
    page  = meta.get("page_number") or meta.get("page") or "N/A"
    lvl1  = meta.get("level1")
    lvl2  = meta.get("level2")
    summary_items, figure_rows = parse_summary_and_figures(getattr(doc, "page_content", "") or "")

    tags_html = '<div class="doc-tags">'
    tags_html += f'<span class="filepill">{html.escape(fname)}</span>'
    if lvl1: tags_html += f' <span class="badge">{html.escape(str(lvl1))}</span>'
    if lvl2: tags_html += f' <span class="badge">{html.escape(str(lvl2))}</span>'
    tags_html += f' <span class="badge">p.{html.escape(str(page))}</span></div>'

    if summary_items:
        items_html = "".join([f"<li>{html.escape(it)}</li>" for it in summary_items])
        sum_html = f'<ul class="sum-list">{items_html}</ul>'
    else:
        sum_html = '<div class="small">ìš”ì•½ ì •ë³´ ì—†ìŒ</div>'

    if figure_rows:
        header = "<tr><th>ê°’</th><th>ë‹¨ìœ„</th><th>ê¸°ê°„/ê¸°ì¤€</th><th>ê·¼ê±°ì˜ì—­</th></tr>"
        rows = ""
        for r in figure_rows:
            rows += (
                "<tr>"
                f"<td>{html.escape(r.get('ê°’',''))}</td>"
                f"<td>{html.escape(r.get('ë‹¨ìœ„',''))}</td>"
                f"<td>{html.escape(r.get('ê¸°ê°„/ê¸°ì¤€',''))}</td>"
                f"<td>{html.escape(r.get('ê·¼ê±°ì˜ì—­',''))}</td>"
                "</tr>"
            )
        fig_html = f'<table class="kv-table">{header}{rows}</table>'
    else:
        fig_html = '<div class="small">í•µì‹¬ ìˆ˜ì¹˜ ì—†ìŒ</div>'

    html_block = (
        f'<div class="doc-card">'
        f'<div class="doc-head">[{idx}] {html.escape(fname)} <span class="small">(p.{html.escape(str(page))})</span></div>'
        f'{tags_html}'
        f'<div class="small" style="margin-top:4px;"><b>í•µì‹¬ìš”ì•½</b></div>'
        f'{sum_html}'
        f'<div class="small" style="margin-top:8px;"><b>í•µì‹¬ìˆ˜ì¹˜</b></div>'
        f'{fig_html}'
        f'</div>'
    )
    st.markdown(html_block, unsafe_allow_html=True)

# -----------------------------
# ğŸ¨ í˜ì´ì§€ ìŠ¤íƒ€ì¼
# -----------------------------
st.set_page_config(page_title="RAG Q&A", page_icon="ğŸ”", layout="wide")

CUSTOM_CSS = """
<style>
header {visibility: hidden;}
.answer-card { border-radius: 16px; padding: 18px 20px; background: #ffffff;
  border: 1px solid rgba(0,0,0,0.08); box-shadow: 0 6px 20px rgba(0,0,0,0.06); }
.chip { display:inline-block; padding:4px 10px; border-radius:999px; background:#eef2ff;
  color:#3949ab; font-weight:600; font-size:12px; margin-bottom:6px; }
.doc-card { border-radius: 14px; padding: 12px 14px; background:#fafafa; border:1px solid #eee; margin-bottom:12px; }
.doc-head {font-weight:700; font-size:14px;}
.doc-body {font-size:13px; color:#333;}
.small {color:#666; font-size:12px;}
.footer {color:#777; font-size:12px; margin-top:8px;}
.stat {color:#444; font-size:13px; margin-bottom:6px;}
.history-card { border-radius: 12px; padding:10px 12px; background:#f6f7fb; border:1px solid #e6e8f5; margin-bottom:8px; }
.history-q {font-weight:700; font-size:13px;}
.history-a {font-size:12px; color:#555; margin-top:4px;}
.suggest-box {margin-top:8px; padding:10px 12px; background:#f9fafb; border:1px dashed #d7dbe2; border-radius:12px;}
.doc-tags {margin:6px 0 8px 0}
.badge {display:inline-block; padding:2px 8px; border-radius:999px; background:#eef2ff; color:#3949ab; font-size:11px; margin-right:6px;}
.filepill {display:inline-block; padding:2px 8px; background:#f1f5ff; border:1px solid #dfe6ff; color:#2146b7; border-radius:8px; font-weight:600;}
.sum-list {margin:6px 0 0 0; padding-left:18px;}
.kv-table {width:100%; border-collapse:collapse; margin-top:8px; font-size:12px;}
.kv-table th, .kv-table td {border:1px solid #e9e9ef; padding:6px 8px; text-align:left;}
.kv-table th {background:#f6f7fb; font-weight:700;}
</style>
"""
st.markdown(CUSTOM_CSS, unsafe_allow_html=True)

st.title("ğŸ” RAG Q&A")
st.caption("03_RAG íŒŒì´í”„ë¼ì¸ì„ Streamlit UIë¡œ â€” LangChain + Chroma + OpenRouter")

# -----------------------------
# ğŸ—‚ï¸ ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
# -----------------------------
for key, default in {
    "history": [],
    "retrieved_docs": [],
    "answer": "",
    "user_query": "",
    "pending_query": None,
    "db_built": False,  # í¬ì¸í„° í™•ì¸/ì¬ë¹Œë“œ ì—¬ë¶€
}.items():
    if key not in st.session_state:
        st.session_state[key] = default

# -----------------------------
# ğŸ§° ì‚¬ì´ë“œë°” (ì„¤ì • + ìµœê·¼ ê¸°ë¡ + ì¸ë±ìŠ¤ ì œì–´)
# -----------------------------
with st.sidebar:
    st.subheader("Settings")

    chroma_dir  = st.text_input("Chroma root directory (pointer)", value=DEFAULT_CHROMA_DIR)
    pkl_path    = st.text_input("PKL ê²½ë¡œ(ìƒ˜í”Œ)", value=str(BASE_DIR / "data" / "df_sample_pages.pkl"))

    top_k       = st.slider("Top-K (retrieval)", 1, 10, 3)
    temperature = st.slider("LLM Temperature", 0.0, 1.0, 0.2)
    model_name  = st.text_input("LLM (OpenRouter)", value=DEFAULT_MODEL)
    embed_name  = st.text_input("Embedding model", value=EMBED_MODEL_NAME)

    # CPU ê³ ì • ê¶Œì¥: ê¸°ë³¸ê°’ì„ CPUë¡œ
    device_opt  = st.selectbox("Embedding device", ["cpu", "auto (cuda if available)", "cuda"], index=0)

    st.caption("â€» OPENROUTER_API_KEY í™˜ê²½ë³€ìˆ˜ ì„¤ì • ì‹œ LLM í˜¸ì¶œ í™œì„±í™”")
    api_key = os.environ.get("OPENROUTER_API_KEY", "")

    st.divider()
    st.subheader("Index")
    rebuild_on_start = st.checkbox("ì•± ì‹œì‘ ì‹œ ê°•ì œ ì¬ìƒì„±", value=False)
    rebuild_now = st.button("ğŸ”¨ ì¸ë±ìŠ¤ ì¬ìƒì„± (ìˆ˜ë™)")

    st.divider()
    st.subheader("ğŸ•˜ Recent (last 5)")
    if not st.session_state["history"]:
        st.caption("ìµœê·¼ Q&A ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")
    else:
        for idx, item in enumerate(st.session_state["history"][:5]):
            q_short = textwrap.shorten(item["q"], width=60, placeholder=" â€¦")
            a_short = textwrap.shorten(item.get("a",""), width=70, placeholder=" â€¦") if item.get("a") else "(ìƒì„± ì—†ìŒ)"
            st.markdown(
                f'<div class="history-card"><div class="history-q">Q: {html.escape(q_short)}</div>'
                f'<div class="history-a">A: {html.escape(a_short)}</div></div>',
                unsafe_allow_html=True
            )
            if st.button("â†º ì´ ì§ˆë¬¸ ë¶ˆëŸ¬ì˜¤ê¸°", key=f"load_hist_{idx}"):
                st.session_state["pending_query"] = item["q"]
                st.rerun()

# -----------------------------
# ğŸ§± DB ì¬ìƒì„± ìœ í‹¸ (ì‹¤ì‹œê°„ ë¡œê·¸/í¬ì¸í„° ê°±ì‹ )
# -----------------------------
def _device_arg(choice: str) -> str:
    if choice.startswith("auto"):
        return "auto"
    return choice  # "cpu"/"cuda"

def rebuild_db_always(pkl_path: str, chroma_dir: str, collection: str, embed_model: str, device_choice: str):
    """
    02_Vectorstore.pyê°€ chroma_dir(ROOT) ì•„ë˜ ìƒˆ ë²„ì „ í´ë”ë¥¼ ë§Œë“¤ê³ 
    CURRENT.txt í¬ì¸í„°ë§Œ ê°±ì‹ . ì§„í–‰ ë¡œê·¸ë¥¼ ì‹¤ì‹œê°„ ì¶œë ¥.
    """
    # ìºì‹œ ë¦¬ì†ŒìŠ¤ í•´ì œ (íŒŒì¼ í•¸ë“¤/ë½ í•´ì œ)
    try:
        st.cache_resource.clear()
    except Exception:
        pass

    vs_script = Path(__file__).resolve().parent / "02_Vectorstore.py"
    if not vs_script.is_file():
        raise FileNotFoundError(f"02_Vectorstore.pyë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {vs_script}")

    env = os.environ.copy()
    env.pop("CHROMA_DB_IMPL", None)          # chroma 1.xì—ì„  ë¶ˆí•„ìš”
    env["TOKENIZERS_PARALLELISM"] = "false"

    cmd = [
        sys.executable, str(vs_script),
        "--root", chroma_dir,
        "--collection", collection,
        "--embed_model", embed_model,
        "--device", _device_arg(device_choice),
        "--pkl", pkl_path,
    ]
    cmd_str = " ".join(cmd)

    # st.status ì§€ì› ì—¬ë¶€
    use_status = hasattr(st, "status")
    log_box = st.empty()  # ëŒ€ì²´ UIìš©

    def _append_log(lines, new_line):
        lines.append(new_line.rstrip())
        # ìµœì‹  300ì¤„ë§Œ í‘œì‹œ
        pre = "<pre style='max-height:280px;overflow:auto;'>" + html.escape("\n".join(lines[-300:])) + "</pre>"
        log_box.markdown(pre, unsafe_allow_html=True)

    if use_status:
        with st.status("ì´ˆê¸° ë²¡í„° DB ì¬ìƒì„± ì¤‘â€¦(í¬ì¸í„° ê°±ì‹ )", expanded=True) as status:
            status.write("1) ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ")
            status.write(f"2) ë¹Œë“œ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰: `{cmd_str}`")

            lines = []
            proc = subprocess.Popen(
                cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,
                text=True, bufsize=1, env=env
            )
            for line in proc.stdout:
                _append_log(lines, line)
            rc = proc.wait()

            if rc != 0:
                out_str = "\n".join(lines)
                status.update(state="error", label="ë¹Œë“œ ì‹¤íŒ¨")
                raise RuntimeError(
                    "02_Vectorstore ì‹¤í–‰ ì‹¤íŒ¨\n"
                    f"CMD: {cmd_str}\n"
                    f"STDOUT/ERR:\n{out_str}"
                )

            status.write("3) í¬ì¸í„°(CURRENT.txt) ê°±ì‹  í™•ì¸ ì¤‘â€¦")
            pointer = Path(chroma_dir) / "CURRENT.txt"
            if not pointer.exists():
                status.update(state="error", label="í¬ì¸í„° íŒŒì¼ ëˆ„ë½")
                raise FileNotFoundError(f"í¬ì¸í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {pointer}")

            store_path = pointer.read_text(encoding="utf-8").strip()
            if not store_path or not Path(store_path).exists():
                status.update(state="error", label="í¬ì¸í„° ê²½ë¡œ ë¶ˆëŸ‰")
                raise FileNotFoundError(f"í¬ì¸í„°ê°€ ê°€ë¦¬í‚¤ëŠ” ê²½ë¡œê°€ ì—†ìŠµë‹ˆë‹¤: {store_path}")

            status.update(state="complete", label="ì´ˆê¸° ë²¡í„° DB ì¬ìƒì„± ì™„ë£Œ")
            return True
    else:
        with st.spinner("ì´ˆê¸° ë²¡í„° DB ì¬ìƒì„± ì¤‘â€¦(í¬ì¸í„° ê°±ì‹ )"):
            lines = []
            proc = subprocess.Popen(
                cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,
                text=True, bufsize=1, env=env
            )
            for line in proc.stdout:
                _append_log(lines, line)
            rc = proc.wait()
            if rc != 0:
                out_str = "\n".join(lines)
                raise RuntimeError(
                    "02_Vectorstore ì‹¤í–‰ ì‹¤íŒ¨\n"
                    f"CMD: {cmd_str}\n"
                    f"STDOUT/ERR:\n{out_str}"
                )

            pointer = Path(chroma_dir) / "CURRENT.txt"
            if not pointer.exists():
                raise FileNotFoundError(f"í¬ì¸í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {pointer}")

            store_path = pointer.read_text(encoding="utf-8").strip()
            if not store_path or not Path(store_path).exists():
                raise FileNotFoundError(f"í¬ì¸í„°ê°€ ê°€ë¦¬í‚¤ëŠ” ê²½ë¡œê°€ ì—†ìŠµë‹ˆë‹¤: {store_path}")

            st.success("ì´ˆê¸° ë²¡í„° DB ì¬ìƒì„± ì™„ë£Œ")
            return True

# -----------------------------
# ğŸ ì•± ì‹œì‘ ì‹œì : í¬ì¸í„° í™•ì¸ & ì„ íƒì  ì¬ë¹Œë“œ
# -----------------------------
pointer_ok = _resolve_store_path(DEFAULT_CHROMA_DIR) is not None if 'chroma_dir' not in st.session_state else _resolve_store_path(st.session_state.get('chroma_dir', DEFAULT_CHROMA_DIR)) is not None
need_build = rebuild_on_start or (_resolve_store_path(DEFAULT_CHROMA_DIR) is None)

# ì²« ì§„ì…ì—ì„œë§Œ íŒë‹¨
if not st.session_state["db_built"]:
    if need_build:
        try:
            rebuild_db_always(
                pkl_path=pkl_path,
                chroma_dir=chroma_dir,
                collection="amore_v1",
                embed_model=EMBED_MODEL_NAME,
                device_choice=device_opt
            )
            st.session_state["db_built"] = True
        except Exception as e:
            # ì‹¤íŒ¨í•´ë„ ì•±ì€ ì‚´ë¦¬ê³ , í¬ì¸í„°ê°€ ìˆìœ¼ë©´ ê·¸ê±¸ë¡œ ì‚¬ìš©
            st.error("ì¸ë±ìŠ¤ ì¬ìƒì„± ì‹¤íŒ¨ â€” ì•„ë˜ ë¡œê·¸ í™•ì¸ í›„ í•„ìš” ì‹œ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
            st.exception(e)
            if _resolve_store_path(chroma_dir) is None:
                st.stop()
    else:
        # í¬ì¸í„°ê°€ ì´ë¯¸ ìˆìœ¼ë©´ ë°”ë¡œ ì‚¬ìš©
        st.session_state["db_built"] = True

# ìˆ˜ë™ ì¬ìƒì„± ë²„íŠ¼
if rebuild_now:
    try:
        rebuild_db_always(
            pkl_path=pkl_path,
            chroma_dir=chroma_dir,
            collection="amore_v1",
            embed_model=EMBED_MODEL_NAME,
            device_choice=device_opt
        )
        st.success("ì¸ë±ìŠ¤ë¥¼ ì¬ìƒì„±í–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        st.error("ì¸ë±ìŠ¤ ì¬ìƒì„± ì‹¤íŒ¨")
        st.exception(e)

# -----------------------------
# â™»ï¸ ë¦¬ì†ŒìŠ¤ ë¡œë“œ (cache)
# -----------------------------
def _normalize_device(dev_choice: str) -> str:
    if dev_choice.startswith("auto"):
        try:
            import torch
            return "cuda" if torch.cuda.is_available() else "cpu"
        except Exception:
            return "cpu"
    return dev_choice  # "cpu"/"cuda"

@st.cache_resource(show_spinner=True)
def load_embeddings(model_name: str, device_choice: str):
    dev = _normalize_device(device_choice)
    return HuggingFaceEmbeddings(
        model_name=model_name,
        model_kwargs={"device": dev},
        encode_kwargs={"normalize_embeddings": True},
    )

@st.cache_resource(show_spinner=True)
def load_vectorstore(persist_root: str, embed_model: str, device_choice: str, collection: str = "amore_v1"):
    """
    í¬ì¸í„° íŒŒì¼(ROOT/CURRENT.txt)ì„ ì½ì–´ ì‹¤ì œ ë²„ì „ í´ë” ê²½ë¡œë¡œ PersistentClient ì—°ê²°
    """
    emb = load_embeddings(embed_model, device_choice)

    pointer = Path(persist_root) / "CURRENT.txt"
    if not pointer.exists():
        raise FileNotFoundError(
            f"í¬ì¸í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {pointer}\n"
            f"ì‚¬ì´ë“œë°”ì˜ 'ì¸ë±ìŠ¤ ì¬ìƒì„±'ìœ¼ë¡œ ë¨¼ì € ë§Œë“¤ì„¸ìš”."
        )
    store_path = pointer.read_text(encoding="utf-8").strip()
    if not store_path or not Path(store_path).exists():
        raise FileNotFoundError(
            f"í¬ì¸í„°ê°€ ê°€ë¦¬í‚¤ëŠ” ê²½ë¡œê°€ ì—†ìŠµë‹ˆë‹¤: {store_path}\n"
            f"ì‚¬ì´ë“œë°”ì—ì„œ ì¸ë±ìŠ¤ë¥¼ ë‹¤ì‹œ ìƒì„±í•˜ì„¸ìš”."
        )

    client = chromadb.PersistentClient(path=store_path)  # 1.x ë°©ì‹
    db = Chroma(client=client, embedding_function=emb, collection_name=collection)
    return db, emb

@st.cache_resource(show_spinner=True)
def build_chain(model_name: str, temperature: float, base_url: str, api_key: str):
    prompt = ChatPromptTemplate.from_messages(
        [("system", SYSTEM_PROMPT), ("user", USER_PROMPT_TEMPLATE)]
    )
    llm = ChatOpenAI(
        model=model_name,
        temperature=temperature,
        base_url=base_url,
        api_key=api_key,
    )
    parser = StrOutputParser()
    return prompt | llm | parser

# -----------------------------
# ğŸ”„ ìœ„ì ¯ ìƒì„± ì „ í”„ë¦¬í•„ ì²˜ë¦¬
# -----------------------------
if st.session_state.get("pending_query"):
    st.session_state["user_query"] = st.session_state.pop("pending_query")

# -----------------------------
# ğŸ” ê²€ìƒ‰ + ìƒì„±
# -----------------------------
query = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”", key="user_query")

st.markdown(
    """
    <div class="suggest-box">
      <b>ì§ˆë¬¸ ì˜ˆ</b><br/>
      1. ì½”ë¡œë‚˜19 ì´í›„ ì†Œë¹„ì íŠ¸ë Œë“œì— ëŒ€í•´ ì•Œë ¤ì¤˜<br/>
      2. ê¸°ëŠ¥ì„± í™”ì¥í’ˆì˜ ì‹œì¥ ê·œëª¨ì™€ ì „ë§ì„ ì•Œë ¤ì¤˜<br/>
      3. 2024ë…„ ë·°í‹° ì‚°ì—… ì£¼ìš”íŠ¸ë Œë“œê°€ ë­ì•¼?
    </div>
    """,
    unsafe_allow_html=True
)

col1, col2, col3 = st.columns([1,1,1])
with col1:
    run_btn = st.button("Ask", type="primary")
with col2:
    clear_btn = st.button("Clear (ì´ í™”ë©´ë§Œ)")
with col3:
    clear_hist = st.button("Recent ë¹„ìš°ê¸°")

if clear_btn:
    st.session_state["answer"] = ""
    st.session_state["retrieved_docs"] = []
    st.session_state["user_query"] = ""
    st.rerun()

if clear_hist:
    st.session_state["history"] = []
    st.rerun()

def make_suggestions(q: str, docs):
    suggestions = []
    if q:
        suggestions.append(f"â€˜{q}â€™ì— ëŒ€í•œ í•µì‹¬ ê·¼ê±°ë§Œ í•­ëª©ë³„ë¡œ ìš”ì•½í•´ì¤˜")
        suggestions.append(f"â€˜{q}â€™ì™€ ê´€ë ¨ëœ ë°˜ëŒ€ ê´€ì ì´ë‚˜ ì ì¬ ë¦¬ìŠ¤í¬ëŠ” ë­ê°€ ìˆì–´?")
    src_name = None
    for d in (docs or []):
        meta = getattr(d, "metadata", {}) or {}
        src  = meta.get("source") or meta.get("file") or meta.get("path")
        if src:
            src_name = os.path.basename(src)
            break
    if src_name:
        suggestions.append(f"{os.path.basename(src_name)} ë¬¸ì„œ ê¸°ì¤€ìœ¼ë¡œ êµ¬ì²´ ì‚¬ë¡€/ìˆ˜ì¹˜ë¥¼ ë½‘ì•„ì¤˜")
    else:
        suggestions.append("ê´€ë ¨ ì§€í‘œ/ìˆ˜ì¹˜ë¥¼ í‘œë¡œ ì •ë¦¬í•´ì¤˜")
    uniq = []
    for s in suggestions:
        if s not in uniq:
            uniq.append(s)
        if len(uniq) == 3:
            break
    return uniq

suggestions = make_suggestions(st.session_state.get("user_query",""), st.session_state.get("retrieved_docs", []))
with st.container():
    st.markdown('<div class="suggest-box"><b>í˜¹ì‹œ ì´ëŸ°ê²Œ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?</b></div>', unsafe_allow_html=True)
    scols = st.columns(3)
    for i, s in enumerate(suggestions):
        if s is None:
            continue
        if scols[i].button(f"ğŸ§© {s}", key=f"sugg_{i}"):
            st.session_state["pending_query"] = s
            st.rerun()

if run_btn:
    with st.spinner("ì„ë² ë”©/ë²¡í„°ìŠ¤í† ì–´ ë¡œë”© ì¤‘â€¦"):
        try:
            db, embeddings = load_vectorstore(chroma_dir, embed_name, device_opt, collection="amore_v1")
        except Exception as e:
            st.exception(e)
            st.stop()

    # ë©”íƒ€ ê¸°ë°˜ ì„ë² ë”© ì°¨ì› ê²€ì¦ (í¬ì¸í„°ê°€ ê°€ë¦¬í‚¤ëŠ” ì‹¤ì œ í´ë”ì—ì„œ ì½ê¸°)
    try:
        qdim = len(embeddings.embed_query("ping"))
        pointer = Path(chroma_dir) / "CURRENT.txt"
        store_path = pointer.read_text(encoding="utf-8").strip() if pointer.exists() else None
        meta_f = os.path.join(store_path, "meta.json") if store_path else None
        if meta_f and os.path.exists(meta_f):
            with open(meta_f, "r", encoding="utf-8") as f:
                meta = json.load(f)
            if "dim" in meta and meta["dim"] != qdim:
                st.error(f"ì„ë² ë”© ì°¨ì› ë¶ˆì¼ì¹˜: DB={meta['dim']} vs í˜„ì¬ ëª¨ë¸={qdim}. "
                         f"DB ìƒì„± ì‹œ ì‚¬ìš©í•œ ì„ë² ë”©ê³¼ ë™ì¼í•œ ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”.")
                st.stop()
    except Exception as e:
        st.exception(e)
        st.stop()

    # ì»¬ë ‰ì…˜ ë¬¸ì„œ ìˆ˜
    try:
        count = db._collection.count()
        st.markdown(f'<div class="stat">ğŸ“¦ í˜„ì¬ ì»¬ë ‰ì…˜ ë¬¸ì„œ ìˆ˜: <b>{count}</b></div>', unsafe_allow_html=True)
        if count == 0:
            st.warning("Chroma ì»¬ë ‰ì…˜ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ì¸ë±ì‹± ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
    except Exception:
        pass

    retriever = db.as_retriever(search_kwargs={"k": top_k})

    with st.spinner("ê²€ìƒ‰ ì¤‘â€¦"):
        docs = retriever.invoke(query) if query.strip() else []
        st.session_state["retrieved_docs"] = docs

    # context ë§Œë“¤ê¸°
    ctx_chunks, src_list = [], []
    for i, d in enumerate(docs, start=1):
        head, body, meta = format_doc(d, i)
        ctx_chunks.append(f"{head}\n{body}")
        src = meta.get("source") or meta.get("file") or meta.get("path") or "unknown"
        if src:
            src_list.append(os.path.basename(src))
    context_text = "\n\n---\n\n".join(ctx_chunks) if ctx_chunks else "N/A"

    # LLM í˜¸ì¶œ
    answer_text = ""
    if not api_key:
        st.info("OPENROUTER_API_KEYê°€ ì—†ì–´ LLM í˜¸ì¶œì„ ìƒëµí•©ë‹ˆë‹¤. ê²€ìƒ‰ ê²°ê³¼ë§Œ í‘œì‹œí•©ë‹ˆë‹¤.")
    else:
        try:
            chain = build_chain(model_name, temperature, OPENROUTER_BASE_URL, api_key)
            with st.spinner("ë‹µë³€ ìƒì„± ì¤‘â€¦"):
                answer_text = chain.invoke({"context": context_text, "question": query}) or ""
        except Exception as e:
            st.exception(e)
            answer_text = ""

    st.session_state["answer"] = answer_text

    # íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸(ë§¨ ì•ì— ì¶”ê°€, 5ê°œ ìœ ì§€)
    hist = st.session_state["history"]
    hist.insert(0, {"q": query, "a": answer_text, "k": top_k, "sources": list(dict.fromkeys(src_list))[:5]})
    st.session_state["history"] = hist[:5]
    st.rerun()

# -----------------------------
# ğŸ§  ë‹µë³€ í‘œì‹œ
# -----------------------------
if st.session_state.get("answer"):
    st.markdown('<div class="chip">Answer</div>', unsafe_allow_html=True)
    st.markdown(f'<div class="answer-card">{st.session_state["answer"]}</div>', unsafe_allow_html=True)

# -----------------------------
# ğŸ“š ì°¸ê³  ë¬¸ì„œ í‘œì‹œ
# -----------------------------
if "retrieved_docs" in st.session_state:
    st.markdown("### ğŸ“š ì°¸ê³  ë¬¸ì„œ")
    docs = st.session_state["retrieved_docs"]
    if not docs:
        st.info("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. (DBê°€ ë¹„ì–´ìˆê±°ë‚˜, ì¿¼ë¦¬ì™€ ë¬¸ì„œê°€ ëœ ìœ ì‚¬í•  ìˆ˜ ìˆì–´ìš”)")
    else:
        for i, d in enumerate(docs, start=1):
            render_doc_card(i, d)
