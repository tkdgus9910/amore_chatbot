# -*- coding: utf-8 -*-
"""
Created on Wed Sep 24 15:58:48 2025

@author: tmlab
"""
#%% 01. data load 

import fitz  # PyMuPDF
from PyPDF2 import PdfReader
from pathlib import Path
import json
import pandas as pd 

ROOT_DIR = Path("D:/OneDrive/í”„ë¡œì íŠ¸/250801_ì•„ëª¨ë ˆ/data")

def process_pdfs_in_directory(root_dir_path: str) -> list:
    """
    ì§€ì •ëœ ë””ë ‰í† ë¦¬ì™€ ê·¸ í•˜ìœ„ì˜ ëª¨ë“  PDF íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ í˜ì´ì§€ë³„ ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

    Args:
        root_dir_path (str): ê²€ìƒ‰ì„ ì‹œì‘í•  ìµœìƒìœ„ ë””ë ‰í† ë¦¬ ê²½ë¡œì…ë‹ˆë‹¤.

    Returns:
        list: ê° PDFì˜ ëª¨ë“  í˜ì´ì§€ì— ëŒ€í•œ ë°ì´í„°ê°€ ë‹´ê¸´ ë”•ì…”ë„ˆë¦¬ì˜ ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
              ìœ íš¨í•œ ë””ë ‰í† ë¦¬ê°€ ì•„ë‹ ê²½ìš° ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    processed_pages = []
    ROOT_DIR = Path(root_dir_path)

    if not ROOT_DIR.is_dir():
        print(f"ì˜¤ë¥˜: '{root_dir_path}'ëŠ” ìœ íš¨í•œ ë””ë ‰í† ë¦¬ê°€ ì•„ë‹™ë‹ˆë‹¤.")
        return []

    # ì§€ì •ëœ ê²½ë¡œì™€ ëª¨ë“  í•˜ìœ„ ê²½ë¡œì—ì„œ .pdf íŒŒì¼ì„ ì°¾ìŠµë‹ˆë‹¤.
    for pdf_path in ROOT_DIR.rglob("*.pdf"):
        print(f"\n--- '{pdf_path.name}' íŒŒì¼ ì²˜ë¦¬ ì‹œì‘ ---")
        
        # íŒŒì¼ ê²½ë¡œì˜ ìƒìœ„ ë‘ í´ë” ì´ë¦„ì„ level1, level2ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
        try:
            level1, level2 = pdf_path.parts[-3:-1]
        except IndexError:
            print(f"  ê²½ê³ : '{pdf_path}'ì—ì„œ level1, level2ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œ êµ¬ì¡°ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            level1, level2 = "N/A", "N/A"

        # PyPDF2ë¥¼ ì‚¬ìš©í•˜ì—¬ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        try:
            reader = PdfReader(pdf_path)
            metadata = reader.metadata
        except Exception as e:
            print(f"  ê²½ê³ : '{pdf_path.name}'ì—ì„œ ë©”íƒ€ë°ì´í„°ë¥¼ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            metadata = {} # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹ˆ ë”•ì…”ë„ˆë¦¬ë¡œ ì´ˆê¸°í™”

        # PyMuPDF(fitz)ë¥¼ ì‚¬ìš©í•˜ì—¬ í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        doc = None
        try:
            doc = fitz.open(pdf_path)
            
            # ê° í˜ì´ì§€ë¥¼ ìˆœíšŒí•˜ë©° ì²˜ë¦¬
            for i, page in enumerate(doc):
                page_num = i + 1
                
                # í˜ì´ì§€ì—ì„œ ì§ì ‘ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                direct_text = page.get_text("text").strip()
                text_size = len(direct_text)
                
                print(f"  - {page_num}/{len(doc)} í˜ì´ì§€ ì²˜ë¦¬ ì¤‘... (ê¸€ì ìˆ˜: {text_size})")

                # í…ìŠ¤íŠ¸ ê¸¸ì´ì— ë”°ë¼ multimodal_applied í”Œë˜ê·¸ ì„¤ì •
                multimodal_applied = text_size < 1000
                
                # ì¶”ì¶œëœ ë°ì´í„°ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ì €ì¥
                page_data = {
                    "level1": level1,
                    "level2": level2,
                    "source": pdf_path.name,
                    "page_number": page_num,
                    "page_count": doc.page_count,
                    "text_recognized": text_size,
                    "multimodal_applied": multimodal_applied,
                    "file_path": str(pdf_path),
                    "direct_text": direct_text,
                    'author': metadata.get('/Author'),
                    'creation_date': metadata.get('/CreationDate')
                }
                processed_pages.append(page_data)
        
        except Exception as e:
            print(f"  ì˜¤ë¥˜: '{pdf_path.name}' íŒŒì¼ì„ ì²˜ë¦¬í•˜ëŠ” ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        finally:
            if doc:
                doc.close()
            print(f"--- '{pdf_path.name}' íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ ---")

    return processed_pages

# --- ì‚¬ìš© ì˜ˆì‹œ ---
if __name__ == "__main__":
    # ì•„ë˜ ê²½ë¡œë¥¼ ì‹¤ì œ PDF íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ ê²½ë¡œë¡œ ìˆ˜ì •í•˜ì—¬ ì‚¬ìš©í•˜ì„¸ìš”.
    # ì˜ˆ: "C:/Users/YourUser/Documents/Reports"
    target_directory = ROOT_DIR
    
    # # í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  PDF ì²˜ë¦¬
    all_data = process_pdfs_in_directory(target_directory)

    if all_data:
        print(f"\n\n--- ìµœì¢… ì²˜ë¦¬ ìš”ì•½ ---")
        print(f"ì´ {len(all_data)} í˜ì´ì§€ì˜ ë°ì´í„°ë¥¼ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤.")
        
        # ì²˜ë¦¬ëœ ë°ì´í„° ì¤‘ ì²« ë²ˆì§¸ í˜ì´ì§€ì˜ ë‚´ìš©ì„ ìƒ˜í”Œë¡œ ì¶œë ¥
        print("\nì²« ë²ˆì§¸ í˜ì´ì§€ ë°ì´í„° ìƒ˜í”Œ (ê¸´ í…ìŠ¤íŠ¸ ë‚´ìš©ì€ ìƒëµ):")
        sample_data = all_data[0].copy()
        sample_data['direct_text'] = sample_data['direct_text'][:100] + "..." # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ì˜ë¼ì„œ í‘œì‹œ
        print(json.dumps(sample_data, indent=4, ensure_ascii=False))
        
df = pd.DataFrame(all_data)

#%% 02. EDA - ë¬¸ì„œ ë³„ í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„í¬ í™•ì¸ 

import os
os.environ['KMP_DUPLICATE_LIB_OK']='True'

# ì´ì œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

temp = df.groupby(['level2', 'source'])['text_recognized'].mean()

# ì´í›„ ì‹œê°í™” ì½”ë“œë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
sns.set_theme()
sns.histplot(temp, kde=True, color="red")
plt.show()

temp = df.groupby(['level2', 'source'])['text_recognized'].mean()

# ì´í›„ ì‹œê°í™” ì½”ë“œë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
sns.set_theme()

sns.histplot(temp, bins= 15, kde=True, color="blue")

plt.show()

df['multimodal_applied'] = False  
source_dict = dict(zip(df['source'], df['text_recognized']))
source_list = [k for k,v in source_dict.items() if v < 2000] # 2000ì ë¯¸ë§Œì´ë©´ ë©€í‹°ëª¨ë‹¬ ì ìš©

df['multimodal_applied'] = df['source'].apply(lambda x : True if x in source_list else False) 

#%% 03. API+VQA ê¸°ë°˜ ì´ë¯¸ì§€ í…ìŠ¤íŠ¸ ì²˜ë¦¬

import os
import io
import base64
import requests
from pdf2image import convert_from_path
from PIL import Image

# =========================
# 0) í™˜ê²½ì„¤ì •
# =========================
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")  # í™˜ê²½ë³€ìˆ˜ë¡œ ë³´ê´€ ê¶Œì¥
if not OPENROUTER_API_KEY:
    raise RuntimeError("í™˜ê²½ë³€ìˆ˜ OPENROUTER_API_KEY ë¥¼ ì„¤ì •í•˜ì„¸ìš”.")

OPENROUTER_BASE_URL = "https://openrouter.ai/api/v1"

MODEL = "mistralai/mistral-small-3.2-24b-instruct" # ok
# MODEL = "qwen/qwen2.5-vl-32b-instruct" # ok

HEADERS = {
    "Authorization": f"Bearer {OPENROUTER_API_KEY}",
    "Content-Type": "application/json"
}

# =========================
# 1) ì…ë ¥ ê²½ë¡œ
# =========================
pdf_path = r"D:\OneDrive\í”„ë¡œì íŠ¸\250801_ì•„ëª¨ë ˆ\data\1. í™”ì¥í’ˆ ì‚°ì—… ë™í–¥\1-1. ì†Œë¹„ì íŠ¸ë Œë“œ\(1)[PlayD] VOICE Trend_12ì›”_ë·°í‹°_fin.pdf"
poppler_bin_path = r"C:\poppler-25.07.0\Library\bin"

# =========================
# 2) ìœ í‹¸: PIL ì´ë¯¸ì§€ â†’ data URL (base64)
# =========================
def pil_to_data_url(img: Image.Image, fmt="PNG", max_side=1800) -> str:
    """
    ëª¨ë¸ ì…ë ¥ì— ê³¼ë„í•˜ê²Œ í° ì´ë¯¸ì§€ëŠ” ì„±ëŠ¥/ì†ë„ ì €í•˜ë¥¼ ìœ ë°œí•  ìˆ˜ ìˆì–´
    í•œìª½ ë³€ì„ max_sideë¡œ ë¦¬ì‚¬ì´ì¦ˆ(ì¶•ì†Œ)í•œ í›„ base64 data URLë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """
    img = img.convert("RGB")
    w, h = img.size
    scale = min(max_side / max(w, h), 1.0)
    if scale < 1.0:
        img = img.resize((int(w * scale), int(h * scale)), Image.LANCZOS)

    buf = io.BytesIO()
    img.save(buf, format=fmt)
    b64 = base64.b64encode(buf.getvalue()).decode("utf-8")
    mime = "image/png" if fmt.upper() == "PNG" else "image/jpeg"
    return f"data:{mime};base64,{b64}"

# =========================
# 3) OpenRouter í˜¸ì¶œ í•¨ìˆ˜
# =========================
def ask_openrouter_vision(model: str, question: str, image_data_url: str, temperature: float = 0.0) -> str:
    """
    OpenAI í˜¸í™˜ chat/completions í˜•ì‹ìœ¼ë¡œ ë¹„ì „-ì–¸ì–´ ëª¨ë¸ì— ì§ˆì˜.
    image_urlì—ëŠ” data URL(base64)ì„ ì „ë‹¬.
    """
    url = f"{OPENROUTER_BASE_URL}/chat/completions"
    payload = {
        "model": model,
        "temperature": temperature,
        "messages": [
            {
                "role": "system",
                "content": "You are a helpful assistant for document visual question answering. "
                           "Read the document image carefully and answer concisely in Korean."
            },
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": question},
                    {"type": "image_url", "image_url": {"url": image_data_url}}
                ]
            }
        ]
    }
    resp = requests.post(url, headers=HEADERS, json=payload, timeout=120)
    resp.raise_for_status()
    data = resp.json()
    return data["choices"][0]["message"]["content"].strip()

# ===============================================
# 4) íŒŒì´í”„ë¼ì¸ í•¨ìˆ˜: íŠ¹ì • í˜ì´ì§€ ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬
# ===============================================
def run_doc_vqa_on_pages(
    pdf_path: str,
    poppler_path: str,
    page_numbers: list[int],
    model: str = MODEL,
    question: str = "ì´ í˜ì´ì§€ì˜ ì£¼ìš” ë‚´ìš©ì€ ë¬´ì—‡ì¸ê°€ìš”?",
) -> dict[int, str]:
    """
    ì§€ì •ëœ í˜ì´ì§€ ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸ì— ëŒ€í•´ì„œë§Œ VQAë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.

    Args:
        pdf_path (str): ë¶„ì„í•  PDF íŒŒì¼ ê²½ë¡œ.
        poppler_path (str): Poppler ë°”ì´ë„ˆë¦¬ ê²½ë¡œ.
        page_numbers (list[int]): ë¶„ì„í•  í˜ì´ì§€ ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: [1, 3, 5]).
        model (str): ì‚¬ìš©í•  ë¹„ì „-ì–¸ì–´ ëª¨ë¸ ì´ë¦„.
        question (str): ê° í˜ì´ì§€ì— ë˜ì§ˆ ì§ˆë¬¸.

    Returns:
        dict[int, str]: í˜ì´ì§€ ë²ˆí˜¸ë¥¼ keyë¡œ, ëª¨ë¸ì˜ ë‹µë³€ì„ valueë¡œ ê°–ëŠ” ë”•ì…”ë„ˆë¦¬.
                        ì˜¤ë¥˜ ë°œìƒ ì‹œ ê°’ì€ "error"ê°€ ë©ë‹ˆë‹¤.
    """
    if not os.path.exists(pdf_path):
        print(f"ì˜¤ë¥˜: PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”: {pdf_path}")
        return {}

    try:
        # ì „ì²´ í˜ì´ì§€ë¥¼ í•œë²ˆì— ì´ë¯¸ì§€ë¡œ ë³€í™˜
        images = convert_from_path(pdf_path, poppler_path=poppler_path)
    except Exception as e:
        print(f"PDFâ†’ì´ë¯¸ì§€ ë³€í™˜ ì¤‘ ì˜¤ë¥˜: {e}")
        print("Poppler ê²½ë¡œ(poppler_path)ê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•˜ì„¸ìš”.")
        return {}

    total_pages = len(images)
    print(f"ì´ {total_pages} í˜ì´ì§€ì˜ PDF íŒŒì¼ì„ ì´ë¯¸ì§€ë¡œ ë³€í™˜í–ˆìŠµë‹ˆë‹¤.")
    print(f"ìš”ì²­ëœ í˜ì´ì§€: {page_numbers}")
    print("-" * 40)

    answer_dict = {}

    # ì§€ì •ëœ í˜ì´ì§€ ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ìˆœíšŒ
    for page_num in page_numbers:
        # í˜ì´ì§€ ë²ˆí˜¸ëŠ” 1ë¶€í„°, ë¦¬ìŠ¤íŠ¸ ì¸ë±ìŠ¤ëŠ” 0ë¶€í„° ì‹œì‘í•˜ë¯€ë¡œ ë³€í™˜
        idx = page_num - 1

        # ìœ íš¨í•œ í˜ì´ì§€ ë²ˆí˜¸ì¸ì§€ í™•ì¸
        if not (0 <= idx < total_pages):
            print(f"âš ï¸ ê²½ê³ : {page_num}ë²ˆ í˜ì´ì§€ëŠ” ìœ íš¨í•œ ë²”ìœ„(1-{total_pages})ë¥¼ ë²—ì–´ë‚˜ë¯€ë¡œ ê±´ë„ˆëœë‹ˆë‹¤.")
            print("-" * 40)
            continue

        print(f"ğŸ“„ {page_num} í˜ì´ì§€ ë¶„ì„ ì¤‘...")
        try:
            data_url = pil_to_data_url(images[idx], fmt="PNG", max_side=1800)
            answer = ask_openrouter_vision(model=model, question=question, image_data_url=data_url)
            print(f"ì§ˆë¬¸: {question}")
            print(f"ë‹µë³€: {answer}")
            answer_dict[page_num] = answer

        except requests.HTTPError as http_err:
            print(f"[HTTP ì˜¤ë¥˜] í˜ì´ì§€ {page_num}: {http_err} | ì‘ë‹µ: {getattr(http_err, 'response', None)}")
            answer_dict[page_num] = "error"
        except Exception as e:
            print(f"[ì˜ˆì™¸] í˜ì´ì§€ {page_num}: {e}")
            answer_dict[page_num] = "error"
        finally:
            print("-" * 40)

    return answer_dict



#%% 04. ìƒ˜í”Œ ë°ì´í„° ì¤€ë¹„

sample_list = ["(2024-08-26) EUROPE 5ê°œ êµ­ê°€ì— ëŒ€í•œ ì‚¬ì—… ì˜ì—­ ë¶„ì„",
 "[24.09] ê²½ìŸì‚¬ ì‹ ì œí’ˆ ì¡°ì‚¬ (ì„ ì¼€ì–´)", 
 "[ê¸°ìˆ  ë™í–¥ ë³´ê³ ì„œ]_ìì™¸ì„  ì°¨ë‹¨ ì—°êµ¬ ê¸°ìˆ  ë™í–¥ ë³´ê³ ì„œ_vf", 
 "ë¯¸ë˜ê¸°ìˆ ìœµí•©í™”ì¥í’ˆ_20220121_ì´ì„±ì›",
 "1. ë§ì¶¤í˜•í™”ì¥í’ˆ ê´€ë ¨ ë²•ë ¹_ë°°í¬", 
 "5.+ê´‘ê³ +ì‚¬ì „ìë¬¸+ë°+ëª¨ë‹ˆí„°ë§+ì—…ë¬´+ì•ˆë‚´+", 
 "9.+2025ë…„+í™”ì¥í’ˆ+ì •ì±…ì„¤ëª…íšŒ+ë°œí‘œìë£Œ_í‘œì‹œê´‘ê³ +ìœ„ë°˜ì‚¬ë¡€", 
 "20250124_2024ë…„+í•˜ë°˜ê¸°+ê¸°ëŠ¥ì„±í™”ì¥í’ˆ+ì‹¬ì‚¬+ë°+ë³´ê³ +í˜„í™©",
 "ê¸°ëŠ¥ì„± í™”ì¥í’ˆ",
 "pwc K-ë·°í‹° ì‚°ì—…ì˜ ë³€í™”",]

sample_list = [i+".pdf" for i in sample_list]

df_sample = df.loc[df['source'].apply(lambda x : True if x in sample_list else False),:]

# í•µì‹¬ í˜ì´ì§€ ì¶”ì¶œ

rep_pages = {}
rep_pages[sample_list[0]] = [2,4,5,8,10]
rep_pages[sample_list[1]] = [2,4,8,15,20]
rep_pages[sample_list[2]] = [1,4,6,12,32]
rep_pages[sample_list[3]] = [5,12,31,46,64]
rep_pages[sample_list[4]] = [8,10,12,16,27]
rep_pages[sample_list[5]] = [3,4,6,10,11] #ê´‘ê³  ì‚¬ì „ìë¬¸ 
rep_pages[sample_list[6]] = [2,5,7,12,16] #20250124 
rep_pages[sample_list[7]] = [1,2,3] #20250124
rep_pages[sample_list[8]] = [5,15,22,37,39] #ê¸°ëŠ¥ì„±
rep_pages[sample_list[9]] = [3,5,13,21,31] #ê¸°ëŠ¥ì„±

df_sample_pages = pd.DataFrame()

for file, pages in rep_pages.items() :
    print(file)
    temp_df = df_sample.loc[df_sample['source'] == file, : ]
    temp_df = temp_df.loc[temp_df['page_number'].isin(pages), : ]
    
    df_sample_pages= pd.concat([df_sample_pages, temp_df], axis = 0)

df_sample_ = df_sample.drop_duplicates('source')
file_directory_dict = dict(zip(df_sample_['source'], df_sample_['file_path']))

df_sample_pages = df

#%% 05. iteration

# MODEL = "mistralai/mistral-small-3.2-24b-instruct" # ok
MODEL = "google/gemma-3-27b-it" # ok
# MODEL = "qwen/qwen2.5-vl-32b-instruct" # ok
# MODEL = "meta-llama/llama-4-scout" # ok
# MODEL = "meta-llama/llama-4-maverick" # ok

df_sample_pages['VQA_result_{}'.format(MODEL)] = np.nan

prompt = """ë‹¹ì‹ ì€ ë¬¸ì„œÂ·ìŠ¬ë¼ì´ë“œÂ·ì›¹í˜ì´ì§€ ìŠ¤í¬ë¦°ìƒ·ì„ ì½ëŠ” VQA ë¶„ì„ê°€ì…ë‹ˆë‹¤.
ì´ í•œ í˜ì´ì§€ì—ì„œ (1) í•µì‹¬ ë©”ì‹œì§€ 3ì¤„ê³¼ (2) í•µì‹¬ ìˆ˜ì¹˜ Top-N ì„ ë½‘ì•„ì£¼ì„¸ìš”.

ê·œì¹™:
- ë³´ì´ëŠ” ì •ë³´ë§Œ ì‚¬ìš©, ì¶”ì •/ìƒìƒ ê¸ˆì§€. ì—†ìœ¼ë©´ "ë¯¸í™•ì¸".
- ìˆ˜ì¹˜ëŠ” {ë¼ë²¨, ê°’, ë‹¨ìœ„, ê¸°ê°„/ê¸°ì¤€, ê·¼ê±°ì˜ì—­}ìœ¼ë¡œ ì •ë¦¬.
- ê°’ì€ ì›ë¬¸ ë‹¨ìœ„ ìœ ì§€(í™˜ì‚° ê¸ˆì§€).
- ì¶©ëŒ ìˆ˜ì¹˜ê°€ ìˆìœ¼ë©´ ë” ë‘ë“œëŸ¬ì§„ ìœ„ì¹˜(í—¤ë“œë¼ì¸/êµµì€ê¸€/ìš”ì•½/íƒ€ì¼/í‘œí•©ê³„) ìš°ì„ , ë‚˜ë¨¸ì§€ëŠ” "ë©”ëª¨"ì— ê¸°ë¡.

ì¶œë ¥(í•œêµ­ì–´):
í•µì‹¬ìš”ì•½:
- (ìš”ì•½1)
- (ìš”ì•½2)
- (ìš”ì•½3)

í•µì‹¬ìˆ˜ì¹˜(ìƒìœ„ N=5):
1) ë¼ë²¨: â€¦ | ê°’: â€¦ | ë‹¨ìœ„: â€¦ | ê¸°ê°„/ê¸°ì¤€: â€¦ | ê·¼ê±°ì˜ì—­: â€¦
2) â€¦
ë©”ëª¨: (í•„ìš” ì‹œë§Œ)

"""

df_sample_pages = df_sample_pages.reset_index(drop = 1)

# =========================
# 5) ì‹¤í–‰ ì˜ˆì‹œ
# =========================
if __name__ == "__main__":
    
    pdf_path_list = set(df_sample_pages['file_path'])
    
    for pdf_path in pdf_path_list : 

        # pdf_path = r"D:\OneDrive\í”„ë¡œì íŠ¸\250801_ì•„ëª¨ë ˆ\data\2. ìƒì‚°ê¸°ìˆ  íŠ¸ë Œë“œ\2-5. ìœµí•©ê¸°ìˆ \ë¯¸ë˜ê¸°ìˆ ìœµí•©í™”ì¥í’ˆ_20220121_ì´ì„±ì›.pdf"
    
        df_temp = df_sample_pages.loc[df_sample_pages['file_path'] == pdf_path , : ]
        
        target_pages = list(df_temp['page_number'])
        
        if str(list(df_temp['VQA_result_{}'.format(MODEL)])[0]) != "nan" : 
            print(pdf_path.split('\\')[-1])
            continue
        
        else :
                
            result = run_doc_vqa_on_pages(
                pdf_path=pdf_path,
                poppler_path=poppler_bin_path,
            page_numbers=target_pages,
                model = MODEL,
                question= prompt,)
            
            mask = df_sample_pages['file_path'].eq(pdf_path)
            df_sample_pages.loc[mask, 'VQA_result_{}'.format(MODEL)] = list(result.values())  # resultê°€ ìŠ¤ì¹¼ë¼ë©´ ì „ í–‰ì— ë¸Œë¡œë“œìºìŠ¤íŠ¸


#%% 06. ì €ì¥

import pickle

with open("df_sample_pages.pkl", "wb") as f :
    pickle.dump(df_sample_pages, f)
    

output_directory = "D:/OneDrive/í”„ë¡œì íŠ¸/250801_ì•„ëª¨ë ˆ/ì„¤ë¬¸ìš©ìƒ˜í”Œ/1_VQA/"

# ë¬¸ìì—´ ì—´ì— í•œí•´ì„œ \x01ë§Œ ìš°ì„  ì œê±°/ì¹˜í™˜
for col in df_sample_pages.columns:
    if df_sample_pages[col].dtype == "object":
        df_sample_pages[col] = df_sample_pages[col].str.replace("\x01", " ", regex=False)

df_sample_pages.to_excel(output_directory + "result_test.xlsx")
