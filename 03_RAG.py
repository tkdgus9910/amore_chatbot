# -*- coding: utf-8 -*-
"""
Created on Wed Sep 24 16:25:02 2025

@author: tmlab
"""

#%% 01. ì„ë² ë”© ë° DB ë¡œë“œ
# -*- coding: utf-8 -*-

from pathlib import Path
import os
import requests
from functools import partial

# LangChain ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableLambda, RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# ================================================================
# 1. OpenRouter API ì„¤ì • ë° í˜¸ì¶œ í•¨ìˆ˜
# ================================================================

# â— ì¤‘ìš”: OpenRouter API í‚¤ë¥¼ ì„¤ì •í•˜ì„¸ìš”.
# os.environ["OPENROUTER_API_KEY"] = "sk-or-v1-..."
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")

OPENROUTER_BASE_URL = "https://openrouter.ai/api/v1"
HEADERS = {
    "Authorization": f"Bearer {OPENROUTER_API_KEY}",
    "Content-Type": "application/json"
}

def ask_openrouter(question: str, model: str, temperature: float = 0.1) -> str:
    """
    OpenAI í˜¸í™˜ chat/completions í˜•ì‹ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì–¸ì–´ ëª¨ë¸ì— ì§ˆì˜í•©ë‹ˆë‹¤.
    (RAG íŒŒì´í”„ë¼ì¸ì— í†µí•©í•˜ê¸° ìœ„í•´ íŒŒë¼ë¯¸í„° ìˆœì„œë¥¼ ì¡°ì •í–ˆìŠµë‹ˆë‹¤.)
    """
    url = f"{OPENROUTER_BASE_URL}/chat/completions"
    payload = {
        "model": model,
        "temperature": temperature,
        "messages": [
            {
                "role": "system",
                "content": "You are a helpful assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise. Answer in Korean."
            },
            {
                "role": "user",
                "content": question # RAG í”„ë¡¬í”„íŠ¸ê°€ í¬í•¨ëœ ì „ì²´ ì§ˆë¬¸ì´ ì´ê³³ìœ¼ë¡œ ì „ë‹¬ë©ë‹ˆë‹¤.
            }
        ]
    }
    
    if not OPENROUTER_API_KEY:
        return "ì˜¤ë¥˜: OpenRouter API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

    try:
        resp = requests.post(url, headers=HEADERS, json=payload, timeout=120)
        resp.raise_for_status()
        data = resp.json()
        return data["choices"][0]["message"]["content"].strip()
    except requests.exceptions.RequestException as e:
        return f"API ìš”ì²­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
    except (KeyError, IndexError):
        return f"API ì‘ë‹µ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {resp.text}"

#%% 02. retriever ì¤€ë¹„ 

# ================================================================
# 2. Retriever ì¤€ë¹„ (ê¸°ì¡´ ì½”ë“œ)
# ================================================================

# ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
print("ì„ë² ë”© ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
model_name = "dragonkue/BGE-m3-ko" # í™•ì • 
# model_name = "nlpai-lab/KoE5"
# model_name = "nlpai-lab/KURE-v1"


model_kwargs = {'device': 'cuda'} # GPUê°€ ì—†ë‹¤ë©´ 'cpu'ë¡œ ë³€ê²½
encode_kwargs = {'normalize_embeddings': True}
embeddings = HuggingFaceEmbeddings(
    model_name=model_name,
    model_kwargs=model_kwargs,
    encode_kwargs=encode_kwargs
)

from pathlib import Path
import tempfile
from langchain_community.vectorstores import Chroma


chroma_persist_dir = "chroma_db_V1"
db = Chroma(
    persist_directory=chroma_persist_dir,
    embedding_function=embeddings,
    # collection_name="amore_v1"
)
retriever = db.as_retriever(search_kwargs={"k": 3})
print("DBë¥¼ Retrieverë¡œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.\n")

#%% 03. íŒŒì´í”„ë¼ì¸ êµ¬ì„±

# ================================================================
# 3. RAG íŒŒì´í”„ë¼ì¸ êµ¬ì„± (Context í¬í•¨í•˜ë„ë¡ ìˆ˜ì •)
# ================================================================

# 3-1. Prompt Template ë° LLM ì¤€ë¹„ (ì´ì „ê³¼ ë™ì¼)
template = """
ì£¼ì–´ì§„ ë§¥ë½(Context) ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì„¸ìš”.
ë§¥ë½ì—ì„œ ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´, "ì œê³µëœ ì •ë³´ë§Œìœ¼ë¡œëŠ” ë‹µë³€í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤."ë¼ê³  ë‹µí•˜ì„¸ìš”. ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”.

[ë§¥ë½]
{context}

[ì§ˆë¬¸]
{question}
"""
prompt = ChatPromptTemplate.from_template(template)
selected_model = "google/gemma-2-9b-it"

llm = RunnableLambda(lambda p: ask_openrouter(question=p.to_string(), model=selected_model))

# 3-2. â­ï¸â­ï¸â­ï¸ ìµœì¢… ì²´ì¸ êµ¬ì„± (ê°€ì¥ í° ë³€ê²½ì ) â­ï¸â­ï¸â­ï¸

# ê²€ìƒ‰ëœ ë¬¸ì„œ(context)ë¥¼ í›„ì† ì²´ì¸ì— ì „ë‹¬í•˜ëŠ” í•¨ìˆ˜
def format_docs(docs):
    return "\n\n".join(f"--- ë¬¸ì„œ {i+1} ---\n{doc.page_content}" for i, doc in enumerate(docs))

# 1. ì§ˆë¬¸ì„ ë°›ì•„ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ê³ , contextì™€ questionì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë§Œë“¦
setup_and_retrieval = RunnablePassthrough.assign(
    context=lambda x: format_docs(retriever.invoke(x["question"]))
)

# 2. contextì™€ questionì„ ë°›ì•„ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì²´ì¸
rag_chain_from_docs = (
    prompt
    | llm
    | StrOutputParser()
)

# 3. ìµœì¢…ì ìœ¼ë¡œ contextì™€ answerë¥¼ í•¨ê»˜ ë°˜í™˜í•˜ëŠ” ì²´ì¸
final_chain = setup_and_retrieval | RunnablePassthrough.assign(
    answer=rag_chain_from_docs
)

# 04.RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ì¶œë ¥ ë°©ì‹ ë³€ê²½)

# ================================================================
# 4. RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ì¶œë ¥ ë°©ì‹ ë³€ê²½)
# ================================================================
if __name__ == "__main__":
    query = 'ì½”ë¡œë‚˜19 ì´í›„ ì†Œë¹„ì íŠ¸ë Œë“œì— ëŒ€í•´ ì•Œë ¤ì¤˜'
    print(f"ì§ˆë¬¸: {query}\n")
    print("--- RAG íŒŒì´í”„ë¼ì¸ ë‹µë³€ ìƒì„± ì¤‘ ---")

    # ì²´ì¸ì„ ì‹¤í–‰í•˜ë©´ 'context'ì™€ 'answer'ê°€ í¬í•¨ëœ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜
    result = final_chain.invoke({"question": query})

    # ìµœì¢… ë‹µë³€ ì¶œë ¥
    print("\nâœ… [ìµœì¢… ë‹µë³€]")
    print(result["answer"])

    # ì°¸ê³ í•œ ì›ë¬¸(Context) ì¶œë ¥
    print("\n\nğŸ“š [ì°¸ê³  ì›ë¬¸]")
    print(result["context"])



#%% 05. Retrieval í‰ê°€ ëª¨ë“ˆ (í™•ì¥íŒ)

import numpy as np
import pandas as pd
from datetime import datetime

# ================================================================
# 1. í™•ì¥ëœ í‰ê°€ ë°ì´í„°ì…‹ (3ê°œ ë¬¸ì„œ ëŒ€ìƒ)
# ================================================================

eval_dataset = [
    # ============================================================
    # ë¬¸ì„œ1: ë‘ì½¤ UV ì—‘ìŠ¤í¼íŠ¸ í†¤ì—… ë°€í¬ ë¡œì§€ë¸”ë£¸
    # ============================================================
    # Easy
    {"query": "ë‘ì½¤ UV ì—‘ìŠ¤í¼íŠ¸ í†¤ì—… ë°€í¬ ì„±ë¶„", 
     "keywords": ["ë‘ì½¤", "UV", "í†¤ì—…"], "difficulty": "easy", "doc_id": "lancome_uv_expert"},
    {"query": "ë‘ì½¤ ë¡œì§€ë¸”ë£¸ ê°€ê²© ìš©ëŸ‰", 
     "keywords": ["98,000", "50 mL"], "difficulty": "easy", "doc_id": "lancome_uv_expert"},
    # Medium
    {"query": "EHMC ì œê±°í•œ ì €ìê·¹ ì„ í¬ë¦¼ ì²˜ë°© ì‚¬ë¡€", 
     "keywords": ["EHMC", "ì €ìê·¹"], "difficulty": "medium", "doc_id": "lancome_uv_expert"},
    {"query": "5ì¤‘ ì°¨ë‹¨ í´ë ˆì„ ì„ ì¼€ì–´ ì œí’ˆ", 
     "keywords": ["UVA", "UVB", "ë¯¸ì„¸ë¨¼ì§€", "ë‹´ë°°ì—°ê¸°"], "difficulty": "medium", "doc_id": "lancome_uv_expert"},
    # Hard
    {"query": "EHT BEMT PBSA ì¡°í•© UV í•„í„° ì²˜ë°©", 
     "keywords": ["EHT", "BEMT", "PBSA"], "difficulty": "hard", "doc_id": "lancome_uv_expert"},
    # Realistic
    {"query": "ê²½ìŸì‚¬ í”„ë¦¬ë¯¸ì—„ í†¤ì—… ì„ ë¸”ë¡ ë²¤ì¹˜ë§ˆí‚¹ ìë£Œ", 
     "keywords": ["í†¤ì—…", "ì•ˆí‹°ì—ì´ì§•", "ë©”ì´í¬ì—… ë² ì´ìŠ¤"], "difficulty": "realistic", "doc_id": "lancome_uv_expert"},
    {"query": "ë¯¼ê°ì„± í”¼ë¶€ìš© ì•ˆí‹°ì—ì´ì§• ì„ ì¼€ì–´ ë ˆí¼ëŸ°ìŠ¤", 
     "keywords": ["ë¯¼ê°", "ì•ˆí‹°ì—ì´ì§•", "ì €ìê·¹"], "difficulty": "realistic", "doc_id": "lancome_uv_expert"},
    
    # ============================================================
    # ë¬¸ì„œ2: ê¸€ë¡œë²Œ ì„ ì¼€ì–´ ì‹œì¥ ë™í–¥
    # ============================================================
    # Easy
    {"query": "ê¸€ë¡œë²Œ ì„ ì¼€ì–´ ì‹œì¥ ê·œëª¨ 2028", 
     "keywords": ["134", "ì–µ ë‹¬ëŸ¬", "17.5ì¡°"], "difficulty": "easy", "doc_id": "suncare_market"},
    {"query": "ì„ ì¼€ì–´ ì‹œì¥ ì—°í‰ê·  ì„±ì¥ë¥  CAGR", 
     "keywords": ["8.5%", "3.48%"], "difficulty": "easy", "doc_id": "suncare_market"},
    # Medium
    {"query": "ì„ ì¼€ì–´ ì‹œì¥ ì£¼ìš” ê²½ìŸì‚¬ í˜„í™©", 
     "keywords": ["L'OrÃ©al", "Beiersdorf", "Shiseido", "Johnson"], "difficulty": "medium", "doc_id": "suncare_market"},
    {"query": "ì•¡ì²´ íƒ€ì… ì„ ìŠ¤í¬ë¦° ì‹œì¥ íŠ¸ë Œë“œ", 
     "keywords": ["ì•¡ì²´", "fluid", "í¡ìˆ˜"], "difficulty": "medium", "doc_id": "suncare_market"},
    {"query": "ë¯¸ë„¤ë„ ì„ ìŠ¤í¬ë¦° ë¬´ê¸°ìì°¨ ì„±ì¥ë¥ ", 
     "keywords": ["ë¯¸ë„¤ë„", "167%", "ë¬´ê¸°ìì°¨"], "difficulty": "medium", "doc_id": "suncare_market"},
    # Hard
    {"query": "ê¸€ë¡œë²Œ ì„ ì¼€ì–´ ì œí˜• íŠ¸ë Œë“œ ë¶„ì„", 
     "keywords": ["ì•¡ì²´", "fluid", "ë¯¸ë„¤ë„"], "difficulty": "hard", "doc_id": "suncare_market"},
    # Realistic
    {"query": "ì„ ì¼€ì–´ ì‹œì¥ ì§„ì… ì „ëµ ìˆ˜ë¦½ìš© ê²½ìŸì‚¬ ë°ì´í„°", 
     "keywords": ["L'OrÃ©al", "Shiseido", "CAGR", "ì„±ì¥ë¥ "], "difficulty": "realistic", "doc_id": "suncare_market"},
    {"query": "2028ë…„ ì„ ì¼€ì–´ ì‹œì¥ ì „ë§ ë¦¬í¬íŠ¸", 
     "keywords": ["2028", "134", "8.5%"], "difficulty": "realistic", "doc_id": "suncare_market"},
    
    # ============================================================
    # ë¬¸ì„œ3: UVMune 400 (MCE) ê¸°ìˆ  ë¬¸ì„œ
    # ============================================================
    # Easy
    {"query": "UVMune 400 MCE í•„í„° íŠ¹ì„±", 
     "keywords": ["UVMune", "MCE", "400"], "difficulty": "easy", "doc_id": "uvmune_tech"},
    {"query": "AAHCP ì œí˜• ê¸°ìˆ  SPF íš¨ìœ¨", 
     "keywords": ["AAHCP", "SPF", "40%"], "difficulty": "easy", "doc_id": "uvmune_tech"},
    # Medium
    {"query": "UVA1 ìŠ¤í™íŠ¸ëŸ¼ ì°¨ë‹¨ ê¸°ìˆ  ë™í–¥", 
     "keywords": ["UVA1", "400 nm", "ìŠ¤í™íŠ¸ëŸ¼"], "difficulty": "medium", "doc_id": "uvmune_tech"},
    {"query": "MCE í•„í„° í¡ìˆ˜ íŒŒì¥ íŠ¹ì„±", 
     "keywords": ["MCE", "390 nm", "í¡ìˆ˜"], "difficulty": "medium", "doc_id": "uvmune_tech"},
    # Hard
    {"query": "ì°¨ì„¸ëŒ€ UV í•„í„° UVA1 ë³´í˜¸ ê¸°ìˆ ", 
     "keywords": ["UVA1", "MCE", "400 nm"], "difficulty": "hard", "doc_id": "uvmune_tech"},
    {"query": "ì„ ìŠ¤í¬ë¦° SPF íš¨ìœ¨ í–¥ìƒ ì œí˜• ê¸°ìˆ ", 
     "keywords": ["SPF", "AAHCP", "40%"], "difficulty": "hard", "doc_id": "uvmune_tech"},
    # Realistic
    {"query": "ê²½ìŸì‚¬ ì‹ ê·œ UV í•„í„° ê¸°ìˆ  ë²¤ì¹˜ë§ˆí‚¹", 
     "keywords": ["UVMune", "MCE", "UVA1"], "difficulty": "realistic", "doc_id": "uvmune_tech"},
    {"query": "ë¡œë ˆì•Œ ì„ ì¼€ì–´ ì‹ ê¸°ìˆ  íŠ¹í—ˆ ë¶„ì„", 
     "keywords": ["UVMune", "400 nm", "MCE"], "difficulty": "realistic", "doc_id": "uvmune_tech"},
    {"query": "í”¼ë¶€ ì„¬ìœ ì•„ì„¸í¬ ë³´í˜¸ UV ì°¨ë‹¨ ê¸°ìˆ ", 
     "keywords": ["ì„¬ìœ ì•„ì„¸í¬", "UVA1", "í”¼ë¶€"], "difficulty": "realistic", "doc_id": "uvmune_tech"},
]


# ================================================================
# 2. í‰ê°€ í•¨ìˆ˜ (DataFrame ì¶œë ¥)
# ================================================================

def evaluate_retriever_detailed(retriever, eval_data, k=3):
    """
    ìƒì„¸ í‰ê°€ ì‹¤í–‰ â†’ DataFrame ë°˜í™˜
    """
    results = []
    
    for sample in eval_data:
        query = sample["query"]
        keywords = sample["keywords"]
        
        # ê²€ìƒ‰ ì‹¤í–‰
        docs = retriever.invoke(query)[:k]
        retrieved_text = " ".join([doc.page_content for doc in docs])
        
        # í‚¤ì›Œë“œ ë§¤ì¹­
        matched = [kw for kw in keywords if kw in retrieved_text]
        hit = len(matched) > 0
        
        # Reciprocal Rank ê³„ì‚°
        rr = 0.0
        for rank, doc in enumerate(docs, 1):
            if any(kw in doc.page_content for kw in keywords):
                rr = 1.0 / rank
                break
        
        # ê²°ê³¼ ì €ì¥
        results.append({
            "query": query,
            "doc_id": sample["doc_id"],
            "difficulty": sample["difficulty"],
            "hit": hit,
            "matched_keywords": matched,
            "match_count": len(matched),
            "total_keywords": len(keywords),
            "reciprocal_rank": rr,
            "retrieved_count": len(docs),
        })
    
    return pd.DataFrame(results)


def generate_summary_report(df):
    """
    í‰ê°€ ê²°ê³¼ ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±
    """
    print("\n" + "=" * 70)
    print(f"ğŸ“Š Retrieval í‰ê°€ ë¦¬í¬íŠ¸ ({datetime.now().strftime('%Y-%m-%d %H:%M')})")
    print("=" * 70)
    
    # 1. ì „ì²´ ìš”ì•½
    overall_hr = df["hit"].mean()
    overall_mrr = df["reciprocal_rank"].mean()
    
    print(f"\nğŸ¯ ì „ì²´ ì„±ëŠ¥")
    print(f"   Hit Rate:  {overall_hr:.1%} ({df['hit'].sum()}/{len(df)})")
    print(f"   MRR:       {overall_mrr:.3f}")
    
    # 2. ë‚œì´ë„ë³„ ìš”ì•½
    print(f"\nğŸ“ˆ ë‚œì´ë„ë³„ ì„±ëŠ¥")
    difficulty_summary = df.groupby("difficulty").agg({
        "hit": ["mean", "sum", "count"],
        "reciprocal_rank": "mean"
    }).round(3)
    difficulty_summary.columns = ["hit_rate", "hits", "total", "mrr"]
    
    for diff in ["easy", "medium", "hard", "realistic"]:
        if diff in difficulty_summary.index:
            row = difficulty_summary.loc[diff]
            print(f"   {diff:10s}: HR={row['hit_rate']:.1%} ({int(row['hits'])}/{int(row['total'])}), MRR={row['mrr']:.3f}")
    
    # 3. ë¬¸ì„œë³„ ìš”ì•½
    print(f"\nğŸ“š ë¬¸ì„œë³„ ì„±ëŠ¥")
    doc_summary = df.groupby("doc_id").agg({
        "hit": ["mean", "sum", "count"],
        "reciprocal_rank": "mean"
    }).round(3)
    doc_summary.columns = ["hit_rate", "hits", "total", "mrr"]
    
    for doc_id in doc_summary.index:
        row = doc_summary.loc[doc_id]
        print(f"   {doc_id:20s}: HR={row['hit_rate']:.1%} ({int(row['hits'])}/{int(row['total'])}), MRR={row['mrr']:.3f}")
    
    # 4. ì‹¤íŒ¨ ì¼€ì´ìŠ¤
    failed = df[df["hit"] == False]
    print(f"\nâŒ ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ({len(failed)}ê±´)")
    for _, row in failed.iterrows():
        print(f"   [{row['difficulty']:10s}] {row['query'][:50]}")
    
    # 5. ìš”ì•½ DataFrame ìƒì„±
    summary_df = pd.DataFrame({
        "metric": ["Overall Hit Rate", "Overall MRR", "Easy HR", "Medium HR", "Hard HR", "Realistic HR"],
        "value": [
            overall_hr,
            overall_mrr,
            difficulty_summary.loc["easy", "hit_rate"] if "easy" in difficulty_summary.index else None,
            difficulty_summary.loc["medium", "hit_rate"] if "medium" in difficulty_summary.index else None,
            difficulty_summary.loc["hard", "hit_rate"] if "hard" in difficulty_summary.index else None,
            difficulty_summary.loc["realistic", "hit_rate"] if "realistic" in difficulty_summary.index else None,
        ]
    })
    
    return summary_df, difficulty_summary, doc_summary


# ================================================================
# 3. ì‹¤í–‰
# ================================================================

if __name__ == "__main__":
    
    print("ğŸ” í‰ê°€ ì‹œì‘...\n")
    
    # ìƒì„¸ ê²°ê³¼ DataFrame
    results_df = evaluate_retriever_detailed(retriever, eval_dataset, k=3)
    
    # ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±
    summary_df, diff_summary, doc_summary = generate_summary_report(results_df)
    
    # DataFrame ì €ì¥ (ì„ íƒ)
    # results_df.to_csv("retrieval_eval_detailed.csv", index=False, encoding="utf-8-sig")
    # summary_df.to_csv("retrieval_eval_summary.csv", index=False, encoding="utf-8-sig")
    
    print("\n" + "=" * 70)
    print("ğŸ“‹ ìƒì„¸ ê²°ê³¼ DataFrame (results_df)")
    print("=" * 70)
    print(results_df[["query", "difficulty", "hit", "match_count", "reciprocal_rank"]].to_string())


#%% 06. í•µì‹¬ ìš”ì•½ DataFrame (ë‹¨ì¼)

def generate_summary_df(results_df):
    """
    í•µì‹¬ ì§€í‘œë¥¼ í•˜ë‚˜ì˜ DataFrameìœ¼ë¡œ ì •ë¦¬
    """
    rows = []
    
    # 1. ì „ì²´
    rows.append({
        "category": "total",
        "group": "overall",
        "queries": len(results_df),
        "hits": results_df["hit"].sum(),
        "hit_rate": round(results_df["hit"].mean(), 3),
        "mrr": round(results_df["reciprocal_rank"].mean(), 3),
    })
    
    # 2. ë‚œì´ë„ë³„
    for diff in ["easy", "medium", "hard", "realistic"]:
        subset = results_df[results_df["difficulty"] == diff]
        if len(subset) > 0:
            rows.append({
                "category": "difficulty",
                "group": diff,
                "queries": len(subset),
                "hits": subset["hit"].sum(),
                "hit_rate": round(subset["hit"].mean(), 3),
                "mrr": round(subset["reciprocal_rank"].mean(), 3),
            })
    
    # 3. ë¬¸ì„œë³„
    for doc_id in results_df["doc_id"].unique():
        subset = results_df[results_df["doc_id"] == doc_id]
        rows.append({
            "category": "document",
            "group": doc_id,
            "queries": len(subset),
            "hits": subset["hit"].sum(),
            "hit_rate": round(subset["hit"].mean(), 3),
            "mrr": round(subset["reciprocal_rank"].mean(), 3),
        })
    
    return pd.DataFrame(rows)


# ì‹¤í–‰
if __name__ == "__main__":
    results_df = evaluate_retriever_detailed(retriever, eval_dataset, k=3)
    summary_df = generate_summary_df(results_df)
    print(summary_df.to_string(index=False))


#%% 07. ê²€ìƒ‰ ì „ëµ ë¹„êµ í‰ê°€ (í™•ì¥ ë²„ì „)

from rank_bm25 import BM25Okapi
from sentence_transformers import CrossEncoder
from typing import List
import numpy as np
import pandas as pd
from itertools import product

# ================================================================
# 1. Sparse Retriever (BM25)
# ================================================================

class BM25Retriever:
    def __init__(self, documents):
        self.documents = documents
        self.tokenized = [doc.page_content.split() for doc in documents]
        self.bm25 = BM25Okapi(self.tokenized)
    
    def invoke(self, query: str, k: int = 3) -> List:
        tokens = query.split()
        scores = self.bm25.get_scores(tokens)
        top_idx = np.argsort(scores)[::-1][:k]
        return [self.documents[i] for i in top_idx]


# ================================================================
# 2. Hybrid Retriever (RRF ê¸°ë°˜)
# ================================================================

class HybridRetriever:
    def __init__(self, dense_retriever, sparse_retriever, alpha=0.5):
        self.dense = dense_retriever
        self.sparse = sparse_retriever
        self.alpha = alpha
    
    def invoke(self, query: str, k: int = 3) -> List:
        dense_docs = self.dense.invoke(query)[:k*2]
        sparse_docs = self.sparse.invoke(query, k*2)
        
        # RRF (Reciprocal Rank Fusion)
        scores = {}
        doc_map = {}
        
        for rank, doc in enumerate(dense_docs):
            doc_id = hash(doc.page_content[:100])
            scores[doc_id] = scores.get(doc_id, 0) + self.alpha / (rank + 1)
            doc_map[doc_id] = doc
        
        for rank, doc in enumerate(sparse_docs):
            doc_id = hash(doc.page_content[:100])
            scores[doc_id] = scores.get(doc_id, 0) + (1 - self.alpha) / (rank + 1)
            doc_map[doc_id] = doc
        
        sorted_ids = sorted(scores.keys(), key=lambda x: scores[x], reverse=True)
        return [doc_map[doc_id] for doc_id in sorted_ids[:k]]


# ================================================================
# 3. Reranker Retriever (ë‹¤ì–‘í•œ ëª¨ë¸ ì§€ì›)
# ================================================================

class RerankerRetriever:
    def __init__(self, base_retriever, rerank_model: str):
        self.base = base_retriever
        self.model_name = rerank_model
        self.reranker = CrossEncoder(rerank_model)
    
    def invoke(self, query: str, k: int = 3) -> List:
        # ì´ˆê¸° ê²€ìƒ‰ (3ë°°ìˆ˜)
        try:
            candidates = self.base.invoke(query)[:k*3]
        except:
            candidates = self.base.invoke(query, k*3)
        
        if not candidates:
            return []
        
        # Cross-Encoder ì¬ìˆœìœ„í™”
        pairs = [(query, doc.page_content) for doc in candidates]
        scores = self.reranker.predict(pairs)
        
        ranked = sorted(zip(candidates, scores), key=lambda x: x[1], reverse=True)
        return [doc for doc, _ in ranked[:k]]


# ================================================================
# 4. Reranker ëª¨ë¸ ëª©ë¡
# ================================================================

RERANK_MODELS = {
    "MiniLM-L6": "cross-encoder/ms-marco-MiniLM-L-6-v2",
    "MiniLM-L12": "cross-encoder/ms-marco-MiniLM-L-12-v2", 
    "BGE-reranker": "BAAI/bge-reranker-base",
    "mmarco-mMiniLM": "cross-encoder/mmarco-mMiniLMv2-L12-H384-v1",  # ë‹¤êµ­ì–´
}


# ================================================================
# 5. ì „ëµ ìƒì„±ê¸°
# ================================================================

def create_all_retrievers(dense_retriever, documents):
    """ëª¨ë“  ê²€ìƒ‰ ì „ëµ ì¡°í•© ìƒì„±"""
    
    retrievers = {}
    
    # 1) Baseline: Dense, Sparse
    sparse_retriever = BM25Retriever(documents)
    retrievers["Dense"] = dense_retriever
    retrievers["Sparse (BM25)"] = sparse_retriever
    
    # 2) Hybrid: alpha ë³€í™”
    alphas = [0.3, 0.5, 0.7]
    hybrid_retrievers = {}
    
    for alpha in alphas:
        name = f"Hybrid (Î±={alpha})"
        hybrid = HybridRetriever(dense_retriever, sparse_retriever, alpha=alpha)
        retrievers[name] = hybrid
        hybrid_retrievers[alpha] = hybrid
    
    # 3) Reranker: ë‹¤ì–‘í•œ ëª¨ë¸ Ã— Hybrid alpha ì¡°í•©
    for model_name, model_path in RERANK_MODELS.items():
        try:
            print(f"ğŸ”„ Loading reranker: {model_name}...")
            
            # Dense + Rerank
            retrievers[f"Dense + {model_name}"] = RerankerRetriever(
                dense_retriever, model_path
            )
            
            # Hybrid(0.5) + Rerank
            retrievers[f"Hybrid(0.5) + {model_name}"] = RerankerRetriever(
                hybrid_retrievers[0.5], model_path
            )
            
        except Exception as e:
            print(f"âš ï¸ {model_name} ë¡œë”© ì‹¤íŒ¨: {e}")
    
    return retrievers


# ================================================================
# 6. í‰ê°€ í•¨ìˆ˜
# ================================================================

def evaluate_single(retriever, eval_data, k=3):
    """ë‹¨ì¼ retriever í‰ê°€"""
    hits, rrs = [], []
    
    for sample in eval_data:
        query = sample["query"]
        keywords = sample["keywords"]
        
        try:
            docs = retriever.invoke(query)[:k]
        except:
            docs = retriever.invoke(query, k)
        
        text = " ".join([d.page_content for d in docs])
        
        # Hit
        hit = any(kw in text for kw in keywords)
        hits.append(hit)
        
        # RR
        rr = 0.0
        for rank, doc in enumerate(docs, 1):
            if any(kw in doc.page_content for kw in keywords):
                rr = 1.0 / rank
                break
        rrs.append(rr)
    
    return {
        "hit_rate": round(np.mean(hits), 3),
        "mrr": round(np.mean(rrs), 3),
        "hits": sum(hits),
        "total": len(hits)
    }


def compare_all_retrievers(retrievers_dict, eval_data, k=3):
    """ì „ì²´ retriever ë¹„êµ"""
    results = []
    
    for name, retriever in retrievers_dict.items():
        print(f"ğŸ“Š í‰ê°€ ì¤‘: {name}")
        metrics = evaluate_single(retriever, eval_data, k)
        metrics["retriever"] = name
        results.append(metrics)
    
    df = pd.DataFrame(results)
    df = df[["retriever", "hit_rate", "mrr", "hits", "total"]]
    df = df.sort_values("mrr", ascending=False).reset_index(drop=True)
    
    return df


# ================================================================
# 7. ê²°ê³¼ ë¶„ì„ í•¨ìˆ˜
# ================================================================

def analyze_results(df):
    """ê²°ê³¼ ë¶„ì„ ë° ìš”ì•½"""
    
    print("\n" + "=" * 70)
    print("ğŸ“Š ê²€ìƒ‰ ì „ëµ ë¹„êµ ê²°ê³¼ (MRR ê¸°ì¤€ ì •ë ¬)")
    print("=" * 70)
    print(df.to_string(index=False))
    
    # Best ì „ëµ
    best = df.iloc[0]
    print(f"\nğŸ† Best: {best['retriever']}")
    print(f"   Hit Rate: {best['hit_rate']:.1%}, MRR: {best['mrr']:.3f}")
    
    # ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„
    print("\nğŸ“ˆ ì¹´í…Œê³ ë¦¬ë³„ ìµœê³  ì„±ëŠ¥")
    print("-" * 50)
    
    # Baseline
    baseline = df[df["retriever"].isin(["Dense", "Sparse (BM25)"])]
    if len(baseline) > 0:
        best_base = baseline.sort_values("mrr", ascending=False).iloc[0]
        print(f"Baseline: {best_base['retriever']} (MRR: {best_base['mrr']:.3f})")
    
    # Hybrid
    hybrid = df[df["retriever"].str.contains("Hybrid") & ~df["retriever"].str.contains("\+")]
    if len(hybrid) > 0:
        best_hybrid = hybrid.sort_values("mrr", ascending=False).iloc[0]
        print(f"Hybrid: {best_hybrid['retriever']} (MRR: {best_hybrid['mrr']:.3f})")
    
    # Rerank
    rerank = df[df["retriever"].str.contains("\+")]
    if len(rerank) > 0:
        best_rerank = rerank.sort_values("mrr", ascending=False).iloc[0]
        print(f"Rerank: {best_rerank['retriever']} (MRR: {best_rerank['mrr']:.3f})")
    
    return df


# ================================================================
# 8. ì‹¤í–‰
# ================================================================

if __name__ == "__main__":
    
    print("ğŸš€ ê²€ìƒ‰ ì „ëµ ë¹„êµ í‰ê°€ ì‹œì‘\n")
    
    # ì „ì²´ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
    all_docs = db.get()
    from langchain.schema import Document
    documents = [Document(page_content=text) for text in all_docs["documents"]]
    print(f"ğŸ“š ì´ ë¬¸ì„œ ìˆ˜: {len(documents)}\n")
    
    # ëª¨ë“  retriever ìƒì„±
    print("=" * 50)
    print("ğŸ”§ Retriever êµ¬ì„± ì¤‘...")
    print("=" * 50)
    retrievers = create_all_retrievers(retriever, documents)
    print(f"\nâœ… ì´ {len(retrievers)}ê°œ ì „ëµ êµ¬ì„± ì™„ë£Œ\n")
    
    # ë¹„êµ í‰ê°€
    print("=" * 50)
    print("ğŸ” í‰ê°€ ì‹œì‘...")
    print("=" * 50)
    comparison_df = compare_all_retrievers(retrievers, eval_dataset, k=3)
    
    # ê²°ê³¼ ë¶„ì„
    final_df = analyze_results(comparison_df)
    
    # ì €ì¥
    # comparison_df.to_csv("retrieval_strategy_comparison.csv", index=False, encoding="utf-8-sig")
    
    
    #%% 08. ìµœì¢… RAG íŒŒì´í”„ë¼ì¸ (ìµœì  ì „ëµ ì ìš©)

from rank_bm25 import BM25Okapi
from sentence_transformers import CrossEncoder
from langchain.schema import Document
import numpy as np

# ================================================================
# 1. ìµœì  Retriever êµ¬ì„± (Hybrid + mmarco-mMiniLM)
# ================================================================

class OptimizedRetriever:
    """Hybrid(Î±=0.5) + mmarco-mMiniLM Reranker"""
    
    def __init__(self, dense_retriever, documents, alpha=0.5):
        self.dense = dense_retriever
        self.alpha = alpha
        
        # Sparse (BM25)
        self.documents = documents
        self.tokenized = [doc.page_content.split() for doc in documents]
        self.bm25 = BM25Okapi(self.tokenized)
        
        # Reranker (ë‹¤êµ­ì–´)
        print("ğŸ”„ Loading reranker: mmarco-mMiniLM...")
        self.reranker = CrossEncoder("cross-encoder/mmarco-mMiniLMv2-L12-H384-v1")
        print("âœ… Reranker ë¡œë”© ì™„ë£Œ")
    
    def _sparse_search(self, query: str, k: int) -> list:
        tokens = query.split()
        scores = self.bm25.get_scores(tokens)
        top_idx = np.argsort(scores)[::-1][:k]
        return [self.documents[i] for i in top_idx]
    
    def _hybrid_search(self, query: str, k: int) -> list:
        """RRF ê¸°ë°˜ Hybrid Search"""
        dense_docs = self.dense.invoke(query)[:k*2]
        sparse_docs = self._sparse_search(query, k*2)
        
        scores = {}
        doc_map = {}
        
        for rank, doc in enumerate(dense_docs):
            doc_id = hash(doc.page_content[:100])
            scores[doc_id] = scores.get(doc_id, 0) + self.alpha / (rank + 1)
            doc_map[doc_id] = doc
        
        for rank, doc in enumerate(sparse_docs):
            doc_id = hash(doc.page_content[:100])
            scores[doc_id] = scores.get(doc_id, 0) + (1 - self.alpha) / (rank + 1)
            doc_map[doc_id] = doc
        
        sorted_ids = sorted(scores.keys(), key=lambda x: scores[x], reverse=True)
        return [doc_map[doc_id] for doc_id in sorted_ids[:k*3]]
    
    def invoke(self, query: str, k: int = 3) -> list:
        """Hybrid + Rerank"""
        # 1) Hybrid ê²€ìƒ‰
        candidates = self._hybrid_search(query, k)
        
        if not candidates:
            return []
        
        # 2) Reranking
        pairs = [(query, doc.page_content) for doc in candidates]
        scores = self.reranker.predict(pairs)
        
        ranked = sorted(zip(candidates, scores), key=lambda x: x[1], reverse=True)
        return [doc for doc, _ in ranked[:k]]


# ================================================================
# 2. ìµœì¢… RAG Chain êµ¬ì„±
# ================================================================

def build_final_rag_chain(optimized_retriever, llm_func):
    """ìµœì¢… RAG íŒŒì´í”„ë¼ì¸"""
    
    def format_docs(docs):
        return "\n\n".join(f"--- ë¬¸ì„œ {i+1} ---\n{doc.page_content}" for i, doc in enumerate(docs))
    
    def rag_invoke(query: str) -> dict:
        # 1) Retrieval
        docs = optimized_retriever.invoke(query, k=3)
        context = format_docs(docs)
        
        # 2) Prompt êµ¬ì„±
        prompt = f"""ì£¼ì–´ì§„ ë§¥ë½(Context) ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì„¸ìš”.
ë§¥ë½ì—ì„œ ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´, "ì œê³µëœ ì •ë³´ë§Œìœ¼ë¡œëŠ” ë‹µë³€í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤."ë¼ê³  ë‹µí•˜ì„¸ìš”.
ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”.

[ë§¥ë½]
{context}

[ì§ˆë¬¸]
{query}
"""
        # 3) Generation
        answer = llm_func(prompt)
        
        return {
            "query": query,
            "context": context,
            "answer": answer,
            "source_docs": docs
        }
    
    return rag_invoke


# ================================================================
# 3. ì´ˆê¸°í™” ë° ì‹¤í–‰
# ================================================================
if __name__ == "__main__":
    
    # ë¬¸ì„œ ë¡œë“œ
    all_docs = db.get()
    documents = [Document(page_content=text) for text in all_docs["documents"]]
    
    # ìµœì  Retriever ìƒì„±
    optimized_retriever = OptimizedRetriever(retriever, documents, alpha=0.5)
    
    # RAG Chain êµ¬ì„±
    rag_chain = build_final_rag_chain(
        optimized_retriever,
        lambda q: ask_openrouter(q, model=selected_model)
    )
    
    # í…ŒìŠ¤íŠ¸
    result = rag_chain("AAHCP ê¸°ìˆ ì˜ SPF íš¨ìœ¨ í–¥ìƒ íš¨ê³¼ëŠ”?")
    result = rag_chain("ê²½ìŸì‚¬ ì„ ì¼€ì–´ UV í•„í„° íŠ¸ë Œë“œ ì•Œë ¤ì¤˜")
    result = rag_chain("ìµœê·¼ ì„ ì¼€ì–´ ì‹œì¥ì˜ ì£¼ìš” ê¸°ìˆ  íŠ¸ë Œë“œëŠ”?")
    result = rag_chain("ì„ ì¼€ì–´ ì‹œì¥ ì£¼ìš” ê²½ìŸì‚¬ëŠ”?")
    result = rag_chain("UVMune 400 MCE í•„í„°ì˜ í•µì‹¬ íŠ¹ì§•ì€?")
    
    print("âœ… ë‹µë³€:", result["answer"])


eval_queries = [
    "ì„ ì¼€ì–´ ì‹œì¥ ì£¼ìš” ê²½ìŸì‚¬ëŠ”?",
    "ìµœê·¼ ì„ ì¼€ì–´ ì‹œì¥ì˜ ì£¼ìš” ê¸°ìˆ  íŠ¸ë Œë“œëŠ”?",
    "ê²½ìŸì‚¬ ì„ ì¼€ì–´ UV í•„í„° íŠ¸ë Œë“œ ì•Œë ¤ì¤˜", 
    "UVMune 400 MCE í•„í„°ì˜ í•µì‹¬ íŠ¹ì§•ì€?",
    "AAHCP ê¸°ìˆ ì˜ SPF íš¨ìœ¨ í–¥ìƒ íš¨ê³¼ëŠ”?"
   ]




#%% 11. ë‹¤ì¤‘ Generation ëª¨ë¸ Ã— ë‹¤ì¤‘ Judge ëª¨ë¸ ë¹„êµ í‰ê°€

import json
import re
import pandas as pd
import numpy as np
from typing import List, Dict
from itertools import product
import time

# ================================================================
# 1. ëª¨ë¸ ì„¤ì •
# ================================================================

# Generation ëª¨ë¸ (RAG ë‹µë³€ ìƒì„±ìš©)
GENERATION_MODELS = {
    "Qwen: Qwen3 VL 30B A3B Thinking": "qwen/qwen3-vl-30b-a3b-thinking",
    "OpenAI: gpt-oss-20b": "openai/gpt-oss-20b",
    "Microsoft: Phi 4 Reasoning Plus": "microsoft/phi-4-reasoning-plus",
    "NVIDIA: Llama 3.3 Nemotron Super 49B V1.5" : "nvidia/llama-3.3-nemotron-super-49b-v1.5", 
    "QwQ-32B" : "qwen/qwq-32b"}

# Judge ëª¨ë¸ (í‰ê°€ìš© - ê³ ì„±ëŠ¥)
JUDGE_MODELS = {
    "Gemini-2.5-Pro": "google/gemini-2.5-pro-preview",
    "GPT-4.1": "openai/gpt-4.1",
    "Grok-4": "x-ai/grok-4",
}


# ================================================================
# 2. LLM í˜¸ì¶œ í•¨ìˆ˜
# ================================================================

def call_llm(prompt: str, model: str, temperature: float = 0.1, max_retries: int = 3) -> str:
    """LLM í˜¸ì¶œ (ì¬ì‹œë„ í¬í•¨)"""
    
    for attempt in range(max_retries):
        try:
            response = ask_openrouter(prompt, model=model, temperature=temperature)
            return response
        except Exception as e:
            if attempt < max_retries - 1:
                print(f"      âš ï¸ ì¬ì‹œë„ {attempt + 1}/{max_retries}: {e}")
                time.sleep(2)
            else:
                return f"Error: {e}"
    return "Error: Max retries exceeded"


# ================================================================
# 3. RAG Chain (ëª¨ë¸ êµì²´ ê°€ëŠ¥)
# ================================================================

def create_rag_chain_with_model(retriever, gen_model: str):
    """íŠ¹ì • generation ëª¨ë¸ë¡œ RAG chain ìƒì„±"""
    
    def format_docs(docs):
        return "\n\n".join(f"--- ë¬¸ì„œ {i+1} ---\n{doc.page_content}" for i, doc in enumerate(docs))
    
    def rag_invoke(query: str) -> dict:
        # Retrieval
        docs = retriever.invoke(query, k=3) if hasattr(retriever, 'invoke') else retriever.invoke(query)[:3]
        context = format_docs(docs)
        
        # Prompt
        prompt = f"""ì£¼ì–´ì§„ ë§¥ë½(Context) ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì„¸ìš”.
ë§¥ë½ì—ì„œ ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´, "ì œê³µëœ ì •ë³´ë§Œìœ¼ë¡œëŠ” ë‹µë³€í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤."ë¼ê³  ë‹µí•˜ì„¸ìš”.
ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”.

[ë§¥ë½]
{context}

[ì§ˆë¬¸]
{query}
"""
        # Generation
        answer = call_llm(prompt, model=gen_model)
        
        return {
            "query": query,
            "context": context,
            "answer": answer,
        }
    
    return rag_invoke


# ================================================================
# 4. RAGAS í‰ê°€ í”„ë¡¬í”„íŠ¸
# ================================================================

RAGAS_PROMPT = """ë‹¹ì‹ ì€ RAG ì‹œìŠ¤í…œì˜ ë‹µë³€ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

[Question]
{question}

[Context]
{context}

[Answer]
{answer}

ë‹¤ìŒ 4ê°€ì§€ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš” (ê° 0.0~1.0):

1. Faithfulness (ì¶©ì‹¤ì„±): Answerê°€ Contextì—ë§Œ ê¸°ë°˜í•˜ëŠ”ê°€? (hallucination ì—†ëŠ”ê°€?)
2. Answer Relevancy (ë‹µë³€ ê´€ë ¨ì„±): Answerê°€ Questionì— ì ì ˆíˆ ëŒ€ì‘í•˜ëŠ”ê°€?
3. Context Relevancy (ë§¥ë½ ê´€ë ¨ì„±): Contextê°€ Questionì— ìœ ìš©í•œê°€?
4. Completeness (ì™„ì „ì„±): Contextì˜ í•µì‹¬ ì •ë³´ê°€ Answerì— í¬í•¨ë˜ì—ˆëŠ”ê°€?

ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:
{{"faithfulness": ì ìˆ˜, "answer_relevancy": ì ìˆ˜, "context_relevancy": ì ìˆ˜, "completeness": ì ìˆ˜}}
"""


def parse_ragas_scores(response: str) -> Dict:
    """RAGAS ì ìˆ˜ íŒŒì‹±"""
    try:
        json_match = re.search(r'\{.*?\}', response, re.DOTALL)
        if json_match:
            scores = json.loads(json_match.group())
            # ì ìˆ˜ ì •ê·œí™”
            for key in ["faithfulness", "answer_relevancy", "context_relevancy", "completeness"]:
                scores[key] = min(1.0, max(0.0, float(scores.get(key, 0))))
            scores["ragas_score"] = round(np.mean([
                scores["faithfulness"], 
                scores["answer_relevancy"],
                scores["context_relevancy"],
                scores["completeness"]
            ]), 3)
            return scores
    except:
        pass
    return {
        "faithfulness": 0, "answer_relevancy": 0, 
        "context_relevancy": 0, "completeness": 0, "ragas_score": 0
    }


# ================================================================
# 5. ì „ì²´ í‰ê°€ ì‹¤í–‰ê¸°
# ================================================================

class MultiModelEvaluator:
    """ë‹¤ì¤‘ Generation Ã— ë‹¤ì¤‘ Judge í‰ê°€"""
    
    def __init__(self, retriever, gen_models: Dict, judge_models: Dict):
        self.retriever = retriever
        self.gen_models = gen_models
        self.judge_models = judge_models
    
    def evaluate_single(self, query: str, gen_model_name: str, gen_model_id: str, 
                       judge_model_name: str, judge_model_id: str) -> Dict:
        """ë‹¨ì¼ (query, gen_model, judge_model) ì¡°í•© í‰ê°€"""
        
        # 1) RAG ì‹¤í–‰
        rag_chain = create_rag_chain_with_model(self.retriever, gen_model_id)
        rag_result = rag_chain(query)
        
        # 2) Judge í‰ê°€
        eval_prompt = RAGAS_PROMPT.format(
            question=query,
            context=rag_result["context"],
            answer=rag_result["answer"]
        )
        judge_response = call_llm(eval_prompt, judge_model_id, temperature=0.0)
        scores = parse_ragas_scores(judge_response)
        
        return {
            "query": query,
            "gen_model": gen_model_name,
            "judge_model": judge_model_name,
            "answer": rag_result["answer"][:100] + "...",
            **scores
        }
    
    def run_full_evaluation(self, queries: List[str], 
                           selected_gen_models: List[str] = None,
                           selected_judge_models: List[str] = None) -> pd.DataFrame:
        """ì „ì²´ í‰ê°€ ì‹¤í–‰"""
        
        # ëª¨ë¸ ì„ íƒ
        gen_models = {k: v for k, v in self.gen_models.items() 
                      if selected_gen_models is None or k in selected_gen_models}
        judge_models = {k: v for k, v in self.judge_models.items() 
                        if selected_judge_models is None or k in selected_judge_models}
        
        total = len(queries) * len(gen_models) * len(judge_models)
        print(f"ğŸš€ í‰ê°€ ì‹œì‘: {len(queries)} queries Ã— {len(gen_models)} gen Ã— {len(judge_models)} judge = {total} ì¡°í•©\n")
        
        results = []
        count = 0
        
        for query in queries:
            print(f"\nğŸ“ Query: {query[:40]}...")
            
            for gen_name, gen_id in gen_models.items():
                print(f"   ğŸ¤– Gen: {gen_name}")
                
                for judge_name, judge_id in judge_models.items():
                    count += 1
                    print(f"      [{count}/{total}] Judge: {judge_name}...", end=" ")
                    
                    try:
                        result = self.evaluate_single(
                            query, gen_name, gen_id, judge_name, judge_id
                        )
                        print(f"âœ… RAGAS: {result['ragas_score']:.3f}")
                        results.append(result)
                    except Exception as e:
                        print(f"âŒ Error: {e}")
                        results.append({
                            "query": query,
                            "gen_model": gen_name,
                            "judge_model": judge_name,
                            "answer": f"Error: {e}",
                            "faithfulness": 0, "answer_relevancy": 0,
                            "context_relevancy": 0, "completeness": 0, "ragas_score": 0
                        })
                    
                    time.sleep(0.5)  # Rate limit ë°©ì§€
        
        return pd.DataFrame(results)


# ================================================================
# 6. ê²°ê³¼ ë¶„ì„ í•¨ìˆ˜
# ================================================================

def analyze_multi_model_results(df: pd.DataFrame) -> Dict:
    """ë‹¤ì¤‘ ëª¨ë¸ í‰ê°€ ê²°ê³¼ ë¶„ì„"""
    
    metrics = ["faithfulness", "answer_relevancy", "context_relevancy", "completeness", "ragas_score"]
    
    print("\n" + "=" * 80)
    print("ğŸ“Š ë‹¤ì¤‘ ëª¨ë¸ RAGAS í‰ê°€ ê²°ê³¼")
    print("=" * 80)
    
    # 1. Generation ëª¨ë¸ë³„ í‰ê·  (ì „ì²´ Judge í‰ê· )
    print("\nğŸ¤– Generation ëª¨ë¸ë³„ ì„±ëŠ¥ (Judge í‰ê· )")
    print("-" * 80)
    
    gen_summary = df.groupby("gen_model")[metrics].mean().round(3)
    gen_summary = gen_summary.sort_values("ragas_score", ascending=False)
    print(gen_summary.to_string())
    
    # Best Generation Model
    best_gen = gen_summary.index[0]
    print(f"\n   ğŸ† Best Generation: {best_gen} (RAGAS: {gen_summary.loc[best_gen, 'ragas_score']:.3f})")
    
    # 2. Judge ëª¨ë¸ë³„ í‰ê·  ì ìˆ˜ (í‰ê°€ ê²½í–¥)
    print("\n\nâš–ï¸ Judge ëª¨ë¸ë³„ í‰ê°€ ê²½í–¥")
    print("-" * 80)
    
    judge_summary = df.groupby("judge_model")[metrics].mean().round(3)
    print(judge_summary.to_string())
    
    # 3. Generation Ã— Judge êµì°¨ ë¶„ì„
    print("\n\nğŸ”€ Generation Ã— Judge êµì°¨í‘œ (RAGAS Score)")
    print("-" * 80)
    
    cross_table = df.pivot_table(
        index="gen_model", 
        columns="judge_model", 
        values="ragas_score", 
        aggfunc="mean"
    ).round(3)
    cross_table = cross_table.sort_values(cross_table.columns[0], ascending=False)
    print(cross_table.to_string())
    
    # 4. Judge ê°„ ì¼ì¹˜ë„
    print("\n\nğŸ” Judge ê°„ ì¼ì¹˜ë„ ë¶„ì„")
    print("-" * 80)
    
    judge_agreement = df.groupby(["query", "gen_model"])["ragas_score"].std().mean()
    print(f"   í‰ê·  í‘œì¤€í¸ì°¨: {judge_agreement:.3f}")
    
    if judge_agreement < 0.1:
        print("   â†’ Judge ê°„ ë†’ì€ ì¼ì¹˜ë„ âœ…")
    elif judge_agreement < 0.15:
        print("   â†’ Judge ê°„ ì–‘í˜¸í•œ ì¼ì¹˜ë„")
    else:
        print("   â†’ Judge ê°„ ë‚®ì€ ì¼ì¹˜ë„ âš ï¸ (ì¶”ê°€ ê²€í†  í•„ìš”)")
    
    # 5. ìƒìœ„/í•˜ìœ„ Generation ëª¨ë¸ ìƒì„¸
    print("\n\nğŸ“ˆ Generation ëª¨ë¸ ìˆœìœ„")
    print("-" * 80)
    
    for rank, (model, row) in enumerate(gen_summary.iterrows(), 1):
        bar = "â–ˆ" * int(row["ragas_score"] * 20) + "â–‘" * (20 - int(row["ragas_score"] * 20))
        print(f"   {rank:2d}. {model:25s} {bar} {row['ragas_score']:.3f}")
    
    return {
        "gen_summary": gen_summary,
        "judge_summary": judge_summary,
        "cross_table": cross_table,
        "judge_agreement": judge_agreement,
        "best_gen_model": best_gen
    }


# ================================================================
# 7. í‰ê°€ ì¿¼ë¦¬
# ================================================================


eval_queries = [
    "ì„ ì¼€ì–´ ì‹œì¥ ì£¼ìš” ê²½ìŸì‚¬ëŠ”?",
    "ìµœê·¼ ì„ ì¼€ì–´ ì‹œì¥ì˜ ì£¼ìš” ê¸°ìˆ  íŠ¸ë Œë“œëŠ”?",
    "ê²½ìŸì‚¬ ì„ ì¼€ì–´ UV í•„í„° íŠ¸ë Œë“œ ì•Œë ¤ì¤˜", 
    "UVMune 400 MCE í•„í„°ì˜ í•µì‹¬ íŠ¹ì§•ì€?",
    "AAHCP ê¸°ìˆ ì˜ SPF íš¨ìœ¨ í–¥ìƒ íš¨ê³¼ëŠ”?"]


# ================================================================
# 8. ì‹¤í–‰
# ================================================================

if __name__ == "__main__":
    
    # í‰ê°€ê¸° ìƒì„±
    evaluator = MultiModelEvaluator(
        retriever=optimized_retriever,  # ì•ì„œ êµ¬ì„±í•œ ìµœì  retriever
        gen_models=GENERATION_MODELS,
        judge_models=JUDGE_MODELS
    )
    
    # ì „ì²´ í‰ê°€ (ë˜ëŠ” ì¼ë¶€ ëª¨ë¸ë§Œ ì„ íƒ)
    results_df = evaluator.run_full_evaluation(
        queries=eval_queries,
        # ì¼ë¶€ë§Œ í…ŒìŠ¤íŠ¸í•  ê²½ìš°:
        # selected_gen_models=["Qwen2.5-7B-Instruct", "Llama-3.1-8B", "Gemma-3-4B"],
        # selected_judge_models=["Gemini-2.5-Pro", "Claude-Opus-4.5"],
    )
    
    # ê²°ê³¼ ë¶„ì„
    analysis = analyze_multi_model_results(results_df)
    
    # ì €ì¥
    # results_df.to_csv("multi_model_ragas_evaluation.csv", index=False, encoding="utf-8-sig")
    
    print("\nâœ… í‰ê°€ ì™„ë£Œ")
    
    
    #%%
    
    